{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "Classifying images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 3:\n",
      "Image - Min Value: 4 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3d80f0828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 3\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # define output range\n",
    "    a = 0.0\n",
    "    b = 1.0\n",
    "    \n",
    "    # define input range\n",
    "    x_min = 0\n",
    "    x_max = 255\n",
    "\n",
    "    return a + (((x-x_min)*(b-a))/(x_max-x_min))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(np.array([0,1,2,3,4,5,6,7,8,9])) # map of encodings \n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    train_labels = encoder.transform(x)\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    \n",
    "    # using print to understand the data\n",
    "    # print(x)\n",
    "    # print(train_labels)\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    return train_labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # image_shape is (32,32,3) and of type tuple\n",
    "    \n",
    "    # set the batch to None\n",
    "    image_shape = list(image_shape)\n",
    "    image_shape.insert(0, None)\n",
    "\n",
    "    # nn_input is of form [batch_size, height, width, depth]\n",
    "    image_input = tf.placeholder(tf.float32, shape=image_shape, name=\"x\")\n",
    "    \n",
    "    return image_input\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    n_classes = [n_classes]\n",
    "    n_classes.insert(0, None)\n",
    "    \n",
    "    label_input = tf.placeholder(tf.float32, shape=n_classes, name=\"y\")\n",
    "    \n",
    "    return label_input\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # check the shape of x_tensor\n",
    "    # print(x_tensor)\n",
    "    \n",
    "    # print(conv_num_outputs)\n",
    "    \n",
    "    x = x_tensor\n",
    "    height = conv_ksize[0]\n",
    "    width = conv_ksize[1]\n",
    "    x_shape = x_tensor.get_shape()[3].value\n",
    "    conv_stride_1 = conv_strides[0]\n",
    "    conv_stride_2 = conv_strides[1]\n",
    "    pool_size_1 = pool_ksize[0]\n",
    "    pool_size_2 = pool_ksize[1]\n",
    "    pool_stride_1 = pool_strides[0]\n",
    "    pool_stride_2 = pool_strides[1]\n",
    "    \n",
    "    \n",
    "    # create weights and bias \n",
    "    weights = tf.Variable(tf.random_normal((height, width, x_shape, conv_num_outputs), stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # print(pool_ksize)\n",
    "    \n",
    "    # convolution\n",
    "    conv = tf.nn.conv2d(x, weights, strides=[1, conv_stride_1, conv_stride_2, 1], padding='SAME')\n",
    "\n",
    "    # add bias\n",
    "    conv = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "    # add non-linear activation\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    # add max pooling\n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_size_1, pool_size_2, 1], strides=[1, pool_stride_1, pool_stride_2, 1], padding='SAME')\n",
    "    \n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 10\n",
    "    \n",
    "    # larger kernel instead of (2,2)\n",
    "    conv_ksize = (4, 4)\n",
    "    \n",
    "    # changed conv_strides from (4,4) to consider more data\n",
    "    conv_strides = (2, 2)\n",
    "    \n",
    "    pool_ksize = (2, 2)\n",
    "    \n",
    "    # changed pool_strides from (2,2) to not pool\n",
    "    pool_strides = (1, 1)\n",
    "    \n",
    "    # CONV RELU POOL with depth of 25\n",
    "    conv1 = conv2d_maxpool(x_tensor, conv_num_outputs+15, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # add dropout layer\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "    \n",
    "    # CONV RELU POOL with depth of 50\n",
    "    conv1 = conv2d_maxpool(conv1, conv_num_outputs+25, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # add dropout layer\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "    \n",
    "    # CONV RELU POOL with depth of 100\n",
    "    conv1 = conv2d_maxpool(conv1, conv_num_outputs+50, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # add dropout layer\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv1 = flatten(conv1)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # FC1 with 100 more nodes\n",
    "    conv1 = fully_conn(conv1, conv_num_outputs+200)\n",
    "    \n",
    "    # add dropout layer\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "    \n",
    "    # FC2\n",
    "    conv1 = fully_conn(conv1, conv_num_outputs+200)\n",
    "    \n",
    "    # add dropout layer\n",
    "    conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "    \n",
    "    # FC3\n",
    "    conv1 = fully_conn(conv1, conv_num_outputs+200)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    conv1 = output(conv1, conv_num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return conv1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    valid_loss = session.run(cost, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    \n",
    "    train_loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    train_acc = sess.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    \n",
    "    print('Valid loss: {:.4f} Valid acc: {:.4f} Train loss: {:.4f} Train acc: {:.4f}'.format(\n",
    "                valid_loss,\n",
    "                valid_acc,\n",
    "                train_loss,\n",
    "                train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0976 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Valid loss: 2.2944 Valid acc: 0.1110 Train loss: 2.2988 Train acc: 0.0500\n",
      "Epoch 30, CIFAR-10 Batch 1:  Valid loss: 2.2822 Valid acc: 0.1162 Train loss: 2.2913 Train acc: 0.1000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Valid loss: 2.2618 Valid acc: 0.1114 Train loss: 2.2764 Train acc: 0.1000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Valid loss: 2.2406 Valid acc: 0.1134 Train loss: 2.2666 Train acc: 0.1000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Valid loss: 2.2553 Valid acc: 0.1162 Train loss: 2.2710 Train acc: 0.1250\n",
      "Epoch 34, CIFAR-10 Batch 1:  Valid loss: 2.2078 Valid acc: 0.1122 Train loss: 2.2467 Train acc: 0.1250\n",
      "Epoch 35, CIFAR-10 Batch 1:  Valid loss: 2.2246 Valid acc: 0.1134 Train loss: 2.2586 Train acc: 0.1250\n",
      "Epoch 36, CIFAR-10 Batch 1:  Valid loss: 2.2198 Valid acc: 0.1128 Train loss: 2.2501 Train acc: 0.1500\n",
      "Epoch 37, CIFAR-10 Batch 1:  Valid loss: 2.2234 Valid acc: 0.1146 Train loss: 2.2412 Train acc: 0.1500\n",
      "Epoch 38, CIFAR-10 Batch 1:  Valid loss: 2.2034 Valid acc: 0.1214 Train loss: 2.2342 Train acc: 0.1500\n",
      "Epoch 39, CIFAR-10 Batch 1:  Valid loss: 2.1931 Valid acc: 0.1422 Train loss: 2.2271 Train acc: 0.1500\n",
      "Epoch 40, CIFAR-10 Batch 1:  Valid loss: 2.1761 Valid acc: 0.1352 Train loss: 2.1961 Train acc: 0.1750\n",
      "Epoch 41, CIFAR-10 Batch 1:  Valid loss: 2.1784 Valid acc: 0.1454 Train loss: 2.1849 Train acc: 0.1750\n",
      "Epoch 42, CIFAR-10 Batch 1:  Valid loss: 2.1980 Valid acc: 0.1244 Train loss: 2.2354 Train acc: 0.1500\n",
      "Epoch 43, CIFAR-10 Batch 1:  Valid loss: 2.1658 Valid acc: 0.1360 Train loss: 2.1832 Train acc: 0.2000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Valid loss: 2.1418 Valid acc: 0.1498 Train loss: 2.1629 Train acc: 0.2500\n",
      "Epoch 45, CIFAR-10 Batch 1:  Valid loss: 2.1370 Valid acc: 0.1508 Train loss: 2.1562 Train acc: 0.2250\n",
      "Epoch 46, CIFAR-10 Batch 1:  Valid loss: 2.1484 Valid acc: 0.1574 Train loss: 2.1794 Train acc: 0.2250\n",
      "Epoch 47, CIFAR-10 Batch 1:  Valid loss: 2.1599 Valid acc: 0.1442 Train loss: 2.1669 Train acc: 0.2000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Valid loss: 2.1460 Valid acc: 0.1578 Train loss: 2.1707 Train acc: 0.2000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Valid loss: 2.1116 Valid acc: 0.1772 Train loss: 2.1213 Train acc: 0.2500\n",
      "Epoch 50, CIFAR-10 Batch 1:  Valid loss: 2.0878 Valid acc: 0.1880 Train loss: 2.1140 Train acc: 0.2500\n",
      "Epoch 51, CIFAR-10 Batch 1:  Valid loss: 2.0881 Valid acc: 0.1844 Train loss: 2.0876 Train acc: 0.2250\n",
      "Epoch 52, CIFAR-10 Batch 1:  Valid loss: 2.0920 Valid acc: 0.1792 Train loss: 2.0710 Train acc: 0.2750\n",
      "Epoch 53, CIFAR-10 Batch 1:  Valid loss: 2.0710 Valid acc: 0.1868 Train loss: 2.0291 Train acc: 0.2750\n",
      "Epoch 54, CIFAR-10 Batch 1:  Valid loss: 2.0908 Valid acc: 0.1824 Train loss: 2.0848 Train acc: 0.2250\n",
      "Epoch 55, CIFAR-10 Batch 1:  Valid loss: 2.0634 Valid acc: 0.1950 Train loss: 2.0341 Train acc: 0.3000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Valid loss: 2.0632 Valid acc: 0.1972 Train loss: 2.0475 Train acc: 0.3000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Valid loss: 2.0571 Valid acc: 0.1978 Train loss: 2.0393 Train acc: 0.3000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Valid loss: 2.0623 Valid acc: 0.1960 Train loss: 2.0289 Train acc: 0.3000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Valid loss: 2.0585 Valid acc: 0.2062 Train loss: 2.0552 Train acc: 0.2750\n",
      "Epoch 60, CIFAR-10 Batch 1:  Valid loss: 2.0594 Valid acc: 0.2032 Train loss: 2.0253 Train acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0984 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.0500\n",
      "Epoch  1, CIFAR-10 Batch 3:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.0750\n",
      "Epoch  1, CIFAR-10 Batch 4:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1250\n",
      "Epoch  2, CIFAR-10 Batch 1:  Valid loss: 2.3026 Valid acc: 0.0978 Train loss: 2.3026 Train acc: 0.1000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Valid loss: 2.2851 Valid acc: 0.1034 Train loss: 2.3009 Train acc: 0.0500\n",
      "Epoch  2, CIFAR-10 Batch 3:  Valid loss: 2.2936 Valid acc: 0.1440 Train loss: 2.3024 Train acc: 0.1500\n",
      "Epoch  2, CIFAR-10 Batch 4:  Valid loss: 2.2352 Valid acc: 0.1786 Train loss: 2.2485 Train acc: 0.1000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Valid loss: 2.2697 Valid acc: 0.1598 Train loss: 2.2745 Train acc: 0.2250\n",
      "Epoch  3, CIFAR-10 Batch 1:  Valid loss: 2.2343 Valid acc: 0.2038 Train loss: 2.2830 Train acc: 0.1250\n",
      "Epoch  3, CIFAR-10 Batch 2:  Valid loss: 2.2277 Valid acc: 0.1868 Train loss: 2.1989 Train acc: 0.2000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Valid loss: 2.1778 Valid acc: 0.2150 Train loss: 2.1104 Train acc: 0.3250\n",
      "Epoch  3, CIFAR-10 Batch 4:  Valid loss: 2.0799 Valid acc: 0.2532 Train loss: 2.1021 Train acc: 0.1500\n",
      "Epoch  3, CIFAR-10 Batch 5:  Valid loss: 2.1153 Valid acc: 0.2316 Train loss: 2.0969 Train acc: 0.2500\n",
      "Epoch  4, CIFAR-10 Batch 1:  Valid loss: 2.0794 Valid acc: 0.2760 Train loss: 2.1171 Train acc: 0.2500\n",
      "Epoch  4, CIFAR-10 Batch 2:  Valid loss: 2.0454 Valid acc: 0.2922 Train loss: 2.0504 Train acc: 0.4250\n",
      "Epoch  4, CIFAR-10 Batch 3:  Valid loss: 2.0105 Valid acc: 0.2986 Train loss: 1.9602 Train acc: 0.3250\n",
      "Epoch  4, CIFAR-10 Batch 4:  Valid loss: 1.9240 Valid acc: 0.3266 Train loss: 1.9637 Train acc: 0.2750\n",
      "Epoch  4, CIFAR-10 Batch 5:  Valid loss: 1.9587 Valid acc: 0.3048 Train loss: 1.8977 Train acc: 0.3000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Valid loss: 1.9081 Valid acc: 0.3450 Train loss: 2.0177 Train acc: 0.3500\n",
      "Epoch  5, CIFAR-10 Batch 2:  Valid loss: 1.9329 Valid acc: 0.3314 Train loss: 1.9178 Train acc: 0.4500\n",
      "Epoch  5, CIFAR-10 Batch 3:  Valid loss: 1.8713 Valid acc: 0.3636 Train loss: 1.7446 Train acc: 0.4500\n",
      "Epoch  5, CIFAR-10 Batch 4:  Valid loss: 1.8488 Valid acc: 0.3766 Train loss: 1.9144 Train acc: 0.3250\n",
      "Epoch  5, CIFAR-10 Batch 5:  Valid loss: 1.8915 Valid acc: 0.3422 Train loss: 1.8083 Train acc: 0.3500\n",
      "Epoch  6, CIFAR-10 Batch 1:  Valid loss: 1.8343 Valid acc: 0.3842 Train loss: 1.9460 Train acc: 0.3250\n",
      "Epoch  6, CIFAR-10 Batch 2:  Valid loss: 1.8640 Valid acc: 0.3532 Train loss: 1.8204 Train acc: 0.4500\n",
      "Epoch  6, CIFAR-10 Batch 3:  Valid loss: 1.8218 Valid acc: 0.3864 Train loss: 1.7137 Train acc: 0.4000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Valid loss: 1.7940 Valid acc: 0.4022 Train loss: 1.8847 Train acc: 0.3500\n",
      "Epoch  6, CIFAR-10 Batch 5:  Valid loss: 1.8466 Valid acc: 0.3624 Train loss: 1.7484 Train acc: 0.4000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Valid loss: 1.7637 Valid acc: 0.4108 Train loss: 1.8498 Train acc: 0.3750\n",
      "Epoch  7, CIFAR-10 Batch 2:  Valid loss: 1.7983 Valid acc: 0.3840 Train loss: 1.7194 Train acc: 0.5250\n",
      "Epoch  7, CIFAR-10 Batch 3:  Valid loss: 1.8040 Valid acc: 0.3998 Train loss: 1.6889 Train acc: 0.4000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Valid loss: 1.7663 Valid acc: 0.4108 Train loss: 1.8778 Train acc: 0.3500\n",
      "Epoch  7, CIFAR-10 Batch 5:  Valid loss: 1.7848 Valid acc: 0.3870 Train loss: 1.6910 Train acc: 0.4250\n",
      "Epoch  8, CIFAR-10 Batch 1:  Valid loss: 1.7428 Valid acc: 0.4322 Train loss: 1.8049 Train acc: 0.4250\n",
      "Epoch  8, CIFAR-10 Batch 2:  Valid loss: 1.7431 Valid acc: 0.4238 Train loss: 1.7079 Train acc: 0.5000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Valid loss: 1.7175 Valid acc: 0.4402 Train loss: 1.6301 Train acc: 0.4250\n",
      "Epoch  8, CIFAR-10 Batch 4:  Valid loss: 1.6957 Valid acc: 0.4502 Train loss: 1.8431 Train acc: 0.2750\n",
      "Epoch  8, CIFAR-10 Batch 5:  Valid loss: 1.7257 Valid acc: 0.4122 Train loss: 1.6360 Train acc: 0.5250\n",
      "Epoch  9, CIFAR-10 Batch 1:  Valid loss: 1.7035 Valid acc: 0.4390 Train loss: 1.7652 Train acc: 0.4500\n",
      "Epoch  9, CIFAR-10 Batch 2:  Valid loss: 1.6654 Valid acc: 0.4304 Train loss: 1.5634 Train acc: 0.4750\n",
      "Epoch  9, CIFAR-10 Batch 3:  Valid loss: 1.6560 Valid acc: 0.4528 Train loss: 1.5430 Train acc: 0.4750\n",
      "Epoch  9, CIFAR-10 Batch 4:  Valid loss: 1.6182 Valid acc: 0.4502 Train loss: 1.7531 Train acc: 0.4000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Valid loss: 1.6994 Valid acc: 0.4076 Train loss: 1.6362 Train acc: 0.4500\n",
      "Epoch 10, CIFAR-10 Batch 1:  Valid loss: 1.6084 Valid acc: 0.4584 Train loss: 1.6250 Train acc: 0.4250\n",
      "Epoch 10, CIFAR-10 Batch 2:  Valid loss: 1.6152 Valid acc: 0.4514 Train loss: 1.5405 Train acc: 0.4250\n",
      "Epoch 10, CIFAR-10 Batch 3:  Valid loss: 1.6243 Valid acc: 0.4500 Train loss: 1.4541 Train acc: 0.4500\n",
      "Epoch 10, CIFAR-10 Batch 4:  Valid loss: 1.6427 Valid acc: 0.4328 Train loss: 1.7536 Train acc: 0.3750\n",
      "Epoch 10, CIFAR-10 Batch 5:  Valid loss: 1.6440 Valid acc: 0.4436 Train loss: 1.6072 Train acc: 0.4250\n",
      "Epoch 11, CIFAR-10 Batch 1:  Valid loss: 1.5741 Valid acc: 0.4792 Train loss: 1.5836 Train acc: 0.4750\n",
      "Epoch 11, CIFAR-10 Batch 2:  Valid loss: 1.5826 Valid acc: 0.4642 Train loss: 1.4868 Train acc: 0.4250\n",
      "Epoch 11, CIFAR-10 Batch 3:  Valid loss: 1.5428 Valid acc: 0.4786 Train loss: 1.3569 Train acc: 0.5750\n",
      "Epoch 11, CIFAR-10 Batch 4:  Valid loss: 1.5484 Valid acc: 0.4854 Train loss: 1.6764 Train acc: 0.3500\n",
      "Epoch 11, CIFAR-10 Batch 5:  Valid loss: 1.6358 Valid acc: 0.4520 Train loss: 1.5400 Train acc: 0.5000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Valid loss: 1.5618 Valid acc: 0.4938 Train loss: 1.5771 Train acc: 0.4750\n",
      "Epoch 12, CIFAR-10 Batch 2:  Valid loss: 1.5608 Valid acc: 0.4776 Train loss: 1.4467 Train acc: 0.4750\n",
      "Epoch 12, CIFAR-10 Batch 3:  Valid loss: 1.5423 Valid acc: 0.4982 Train loss: 1.3919 Train acc: 0.5250\n",
      "Epoch 12, CIFAR-10 Batch 4:  Valid loss: 1.4868 Valid acc: 0.4952 Train loss: 1.5613 Train acc: 0.4250\n",
      "Epoch 12, CIFAR-10 Batch 5:  Valid loss: 1.4540 Valid acc: 0.4852 Train loss: 1.3492 Train acc: 0.5250\n",
      "Epoch 13, CIFAR-10 Batch 1:  Valid loss: 1.4126 Valid acc: 0.5066 Train loss: 1.3918 Train acc: 0.5000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Valid loss: 1.4623 Valid acc: 0.5018 Train loss: 1.3608 Train acc: 0.5250\n",
      "Epoch 13, CIFAR-10 Batch 3:  Valid loss: 1.4059 Valid acc: 0.5078 Train loss: 1.2598 Train acc: 0.5500\n",
      "Epoch 13, CIFAR-10 Batch 4:  Valid loss: 1.3889 Valid acc: 0.5254 Train loss: 1.4200 Train acc: 0.4500\n",
      "Epoch 13, CIFAR-10 Batch 5:  Valid loss: 1.4463 Valid acc: 0.4846 Train loss: 1.3449 Train acc: 0.4750\n",
      "Epoch 14, CIFAR-10 Batch 1:  Valid loss: 1.3899 Valid acc: 0.5096 Train loss: 1.3823 Train acc: 0.5500\n",
      "Epoch 14, CIFAR-10 Batch 2:  Valid loss: 1.4036 Valid acc: 0.5170 Train loss: 1.3166 Train acc: 0.5000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Valid loss: 1.3846 Valid acc: 0.5194 Train loss: 1.2009 Train acc: 0.6000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Valid loss: 1.3705 Valid acc: 0.5186 Train loss: 1.4045 Train acc: 0.4500\n",
      "Epoch 14, CIFAR-10 Batch 5:  Valid loss: 1.4403 Valid acc: 0.4834 Train loss: 1.4135 Train acc: 0.5500\n",
      "Epoch 15, CIFAR-10 Batch 1:  Valid loss: 1.3624 Valid acc: 0.5300 Train loss: 1.3393 Train acc: 0.5750\n",
      "Epoch 15, CIFAR-10 Batch 2:  Valid loss: 1.3556 Valid acc: 0.5226 Train loss: 1.2690 Train acc: 0.5000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Valid loss: 1.3672 Valid acc: 0.5248 Train loss: 1.2380 Train acc: 0.6000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Valid loss: 1.3565 Valid acc: 0.5278 Train loss: 1.3955 Train acc: 0.5000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Valid loss: 1.3750 Valid acc: 0.5054 Train loss: 1.2873 Train acc: 0.5750\n",
      "Epoch 16, CIFAR-10 Batch 1:  Valid loss: 1.3334 Valid acc: 0.5432 Train loss: 1.2850 Train acc: 0.5500\n",
      "Epoch 16, CIFAR-10 Batch 2:  Valid loss: 1.3460 Valid acc: 0.5364 Train loss: 1.2610 Train acc: 0.5000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Valid loss: 1.3428 Valid acc: 0.5386 Train loss: 1.1717 Train acc: 0.6250\n",
      "Epoch 16, CIFAR-10 Batch 4:  Valid loss: 1.3187 Valid acc: 0.5292 Train loss: 1.3506 Train acc: 0.5750\n",
      "Epoch 16, CIFAR-10 Batch 5:  Valid loss: 1.3944 Valid acc: 0.5020 Train loss: 1.2812 Train acc: 0.6000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Valid loss: 1.3044 Valid acc: 0.5378 Train loss: 1.1910 Train acc: 0.6000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Valid loss: 1.3362 Valid acc: 0.5328 Train loss: 1.2368 Train acc: 0.5500\n",
      "Epoch 17, CIFAR-10 Batch 3:  Valid loss: 1.3112 Valid acc: 0.5564 Train loss: 1.1230 Train acc: 0.7750\n",
      "Epoch 17, CIFAR-10 Batch 4:  Valid loss: 1.3024 Valid acc: 0.5424 Train loss: 1.2886 Train acc: 0.5750\n",
      "Epoch 17, CIFAR-10 Batch 5:  Valid loss: 1.3482 Valid acc: 0.5178 Train loss: 1.2625 Train acc: 0.6250\n",
      "Epoch 18, CIFAR-10 Batch 1:  Valid loss: 1.3043 Valid acc: 0.5556 Train loss: 1.2275 Train acc: 0.5750\n",
      "Epoch 18, CIFAR-10 Batch 2:  Valid loss: 1.2826 Valid acc: 0.5524 Train loss: 1.2424 Train acc: 0.5500\n",
      "Epoch 18, CIFAR-10 Batch 3:  Valid loss: 1.2854 Valid acc: 0.5528 Train loss: 1.0873 Train acc: 0.6500\n",
      "Epoch 18, CIFAR-10 Batch 4:  Valid loss: 1.2651 Valid acc: 0.5632 Train loss: 1.2330 Train acc: 0.5750\n",
      "Epoch 18, CIFAR-10 Batch 5:  Valid loss: 1.2933 Valid acc: 0.5524 Train loss: 1.1987 Train acc: 0.6250\n",
      "Epoch 19, CIFAR-10 Batch 1:  Valid loss: 1.2764 Valid acc: 0.5596 Train loss: 1.1483 Train acc: 0.6500\n",
      "Epoch 19, CIFAR-10 Batch 2:  Valid loss: 1.2472 Valid acc: 0.5632 Train loss: 1.1852 Train acc: 0.5750\n",
      "Epoch 19, CIFAR-10 Batch 3:  Valid loss: 1.2614 Valid acc: 0.5608 Train loss: 1.0802 Train acc: 0.6000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Valid loss: 1.3518 Valid acc: 0.5220 Train loss: 1.2907 Train acc: 0.5750\n",
      "Epoch 19, CIFAR-10 Batch 5:  Valid loss: 1.2738 Valid acc: 0.5452 Train loss: 1.2122 Train acc: 0.5500\n",
      "Epoch 20, CIFAR-10 Batch 1:  Valid loss: 1.2591 Valid acc: 0.5648 Train loss: 1.1656 Train acc: 0.6500\n",
      "Epoch 20, CIFAR-10 Batch 2:  Valid loss: 1.2463 Valid acc: 0.5726 Train loss: 1.2429 Train acc: 0.5500\n",
      "Epoch 20, CIFAR-10 Batch 3:  Valid loss: 1.2410 Valid acc: 0.5768 Train loss: 1.0195 Train acc: 0.6500\n",
      "Epoch 20, CIFAR-10 Batch 4:  Valid loss: 1.2723 Valid acc: 0.5612 Train loss: 1.2097 Train acc: 0.5750\n",
      "Epoch 20, CIFAR-10 Batch 5:  Valid loss: 1.3179 Valid acc: 0.5320 Train loss: 1.2176 Train acc: 0.6500\n",
      "Epoch 21, CIFAR-10 Batch 1:  Valid loss: 1.2451 Valid acc: 0.5704 Train loss: 1.0611 Train acc: 0.6500\n",
      "Epoch 21, CIFAR-10 Batch 2:  Valid loss: 1.2879 Valid acc: 0.5392 Train loss: 1.2343 Train acc: 0.6000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Valid loss: 1.2499 Valid acc: 0.5734 Train loss: 1.0659 Train acc: 0.7000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Valid loss: 1.2381 Valid acc: 0.5740 Train loss: 1.1769 Train acc: 0.5750\n",
      "Epoch 21, CIFAR-10 Batch 5:  Valid loss: 1.2783 Valid acc: 0.5490 Train loss: 1.1625 Train acc: 0.6250\n",
      "Epoch 22, CIFAR-10 Batch 1:  Valid loss: 1.2253 Valid acc: 0.5782 Train loss: 1.1159 Train acc: 0.6000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Valid loss: 1.2301 Valid acc: 0.5776 Train loss: 1.1804 Train acc: 0.5750\n",
      "Epoch 22, CIFAR-10 Batch 3:  Valid loss: 1.2626 Valid acc: 0.5698 Train loss: 1.0824 Train acc: 0.6750\n",
      "Epoch 22, CIFAR-10 Batch 4:  Valid loss: 1.2503 Valid acc: 0.5630 Train loss: 1.1833 Train acc: 0.5500\n",
      "Epoch 22, CIFAR-10 Batch 5:  Valid loss: 1.2085 Valid acc: 0.5824 Train loss: 1.0816 Train acc: 0.6750\n",
      "Epoch 23, CIFAR-10 Batch 1:  Valid loss: 1.2230 Valid acc: 0.5736 Train loss: 1.0875 Train acc: 0.6000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Valid loss: 1.2410 Valid acc: 0.5698 Train loss: 1.2169 Train acc: 0.5500\n",
      "Epoch 23, CIFAR-10 Batch 3:  Valid loss: 1.1965 Valid acc: 0.5880 Train loss: 1.0098 Train acc: 0.7000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Valid loss: 1.2061 Valid acc: 0.5830 Train loss: 1.0961 Train acc: 0.6750\n",
      "Epoch 23, CIFAR-10 Batch 5:  Valid loss: 1.2624 Valid acc: 0.5582 Train loss: 1.1324 Train acc: 0.7250\n",
      "Epoch 24, CIFAR-10 Batch 1:  Valid loss: 1.1703 Valid acc: 0.5966 Train loss: 1.0137 Train acc: 0.6500\n",
      "Epoch 24, CIFAR-10 Batch 2:  Valid loss: 1.2477 Valid acc: 0.5710 Train loss: 1.1535 Train acc: 0.5750\n",
      "Epoch 24, CIFAR-10 Batch 3:  Valid loss: 1.1776 Valid acc: 0.6006 Train loss: 0.9813 Train acc: 0.7000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Valid loss: 1.1837 Valid acc: 0.5964 Train loss: 1.0841 Train acc: 0.6250\n",
      "Epoch 24, CIFAR-10 Batch 5:  Valid loss: 1.1927 Valid acc: 0.5880 Train loss: 1.1452 Train acc: 0.6500\n",
      "Epoch 25, CIFAR-10 Batch 1:  Valid loss: 1.1849 Valid acc: 0.5958 Train loss: 0.9998 Train acc: 0.7000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Valid loss: 1.1785 Valid acc: 0.5960 Train loss: 1.1354 Train acc: 0.5250\n",
      "Epoch 25, CIFAR-10 Batch 3:  Valid loss: 1.1763 Valid acc: 0.5924 Train loss: 0.9891 Train acc: 0.6750\n",
      "Epoch 25, CIFAR-10 Batch 4:  Valid loss: 1.1756 Valid acc: 0.5934 Train loss: 1.1211 Train acc: 0.6000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Valid loss: 1.1971 Valid acc: 0.5822 Train loss: 1.0575 Train acc: 0.7500\n",
      "Epoch 26, CIFAR-10 Batch 1:  Valid loss: 1.1996 Valid acc: 0.5728 Train loss: 1.0929 Train acc: 0.6250\n",
      "Epoch 26, CIFAR-10 Batch 2:  Valid loss: 1.1822 Valid acc: 0.5942 Train loss: 1.1483 Train acc: 0.5500\n",
      "Epoch 26, CIFAR-10 Batch 3:  Valid loss: 1.1572 Valid acc: 0.5988 Train loss: 0.9429 Train acc: 0.6750\n",
      "Epoch 26, CIFAR-10 Batch 4:  Valid loss: 1.1893 Valid acc: 0.5932 Train loss: 1.1018 Train acc: 0.5750\n",
      "Epoch 26, CIFAR-10 Batch 5:  Valid loss: 1.1683 Valid acc: 0.5962 Train loss: 1.0581 Train acc: 0.7000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Valid loss: 1.1481 Valid acc: 0.6072 Train loss: 0.9391 Train acc: 0.7250\n",
      "Epoch 27, CIFAR-10 Batch 2:  Valid loss: 1.1649 Valid acc: 0.5982 Train loss: 1.1423 Train acc: 0.5000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Valid loss: 1.1401 Valid acc: 0.6072 Train loss: 0.9041 Train acc: 0.7250\n",
      "Epoch 27, CIFAR-10 Batch 4:  Valid loss: 1.1763 Valid acc: 0.5850 Train loss: 1.0586 Train acc: 0.5750\n",
      "Epoch 27, CIFAR-10 Batch 5:  Valid loss: 1.1876 Valid acc: 0.5922 Train loss: 1.1418 Train acc: 0.6250\n",
      "Epoch 28, CIFAR-10 Batch 1:  Valid loss: 1.1417 Valid acc: 0.6078 Train loss: 0.9532 Train acc: 0.7250\n",
      "Epoch 28, CIFAR-10 Batch 2:  Valid loss: 1.1606 Valid acc: 0.6022 Train loss: 1.1250 Train acc: 0.6250\n",
      "Epoch 28, CIFAR-10 Batch 3:  Valid loss: 1.1399 Valid acc: 0.6094 Train loss: 0.9328 Train acc: 0.7500\n",
      "Epoch 28, CIFAR-10 Batch 4:  Valid loss: 1.1458 Valid acc: 0.5954 Train loss: 1.0488 Train acc: 0.6000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Valid loss: 1.1338 Valid acc: 0.6022 Train loss: 1.0470 Train acc: 0.6750\n",
      "Epoch 29, CIFAR-10 Batch 1:  Valid loss: 1.1537 Valid acc: 0.5984 Train loss: 0.9306 Train acc: 0.6500\n",
      "Epoch 29, CIFAR-10 Batch 2:  Valid loss: 1.1608 Valid acc: 0.5928 Train loss: 1.1506 Train acc: 0.6000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Valid loss: 1.1230 Valid acc: 0.6174 Train loss: 0.9070 Train acc: 0.7750\n",
      "Epoch 29, CIFAR-10 Batch 4:  Valid loss: 1.1399 Valid acc: 0.6038 Train loss: 1.0175 Train acc: 0.6750\n",
      "Epoch 29, CIFAR-10 Batch 5:  Valid loss: 1.1489 Valid acc: 0.5976 Train loss: 1.0561 Train acc: 0.7500\n",
      "Epoch 30, CIFAR-10 Batch 1:  Valid loss: 1.1763 Valid acc: 0.5828 Train loss: 1.0300 Train acc: 0.6750\n",
      "Epoch 30, CIFAR-10 Batch 2:  Valid loss: 1.1347 Valid acc: 0.6138 Train loss: 1.1131 Train acc: 0.6250\n",
      "Epoch 30, CIFAR-10 Batch 3:  Valid loss: 1.1365 Valid acc: 0.6130 Train loss: 0.9466 Train acc: 0.7000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Valid loss: 1.1074 Valid acc: 0.6148 Train loss: 1.0151 Train acc: 0.7250\n",
      "Epoch 30, CIFAR-10 Batch 5:  Valid loss: 1.1284 Valid acc: 0.6152 Train loss: 1.0078 Train acc: 0.6750\n",
      "Epoch 31, CIFAR-10 Batch 1:  Valid loss: 1.1341 Valid acc: 0.6110 Train loss: 0.9296 Train acc: 0.7000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Valid loss: 1.1771 Valid acc: 0.6048 Train loss: 1.0526 Train acc: 0.6000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Valid loss: 1.1051 Valid acc: 0.6232 Train loss: 0.8673 Train acc: 0.7250\n",
      "Epoch 31, CIFAR-10 Batch 4:  Valid loss: 1.1376 Valid acc: 0.6166 Train loss: 0.9452 Train acc: 0.6750\n",
      "Epoch 31, CIFAR-10 Batch 5:  Valid loss: 1.1105 Valid acc: 0.6130 Train loss: 1.0146 Train acc: 0.7000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Valid loss: 1.1082 Valid acc: 0.6206 Train loss: 0.9709 Train acc: 0.6750\n",
      "Epoch 32, CIFAR-10 Batch 2:  Valid loss: 1.1210 Valid acc: 0.6198 Train loss: 1.0591 Train acc: 0.5750\n",
      "Epoch 32, CIFAR-10 Batch 3:  Valid loss: 1.0899 Valid acc: 0.6322 Train loss: 0.8405 Train acc: 0.7750\n",
      "Epoch 32, CIFAR-10 Batch 4:  Valid loss: 1.1043 Valid acc: 0.6268 Train loss: 0.9405 Train acc: 0.7000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Valid loss: 1.0987 Valid acc: 0.6110 Train loss: 1.0040 Train acc: 0.7000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Valid loss: 1.0907 Valid acc: 0.6254 Train loss: 0.9326 Train acc: 0.6750\n",
      "Epoch 33, CIFAR-10 Batch 2:  Valid loss: 1.1242 Valid acc: 0.6132 Train loss: 1.1045 Train acc: 0.5750\n",
      "Epoch 33, CIFAR-10 Batch 3:  Valid loss: 1.1193 Valid acc: 0.6132 Train loss: 0.8851 Train acc: 0.6500\n",
      "Epoch 33, CIFAR-10 Batch 4:  Valid loss: 1.1042 Valid acc: 0.6210 Train loss: 0.9278 Train acc: 0.7250\n",
      "Epoch 33, CIFAR-10 Batch 5:  Valid loss: 1.0995 Valid acc: 0.6140 Train loss: 0.9433 Train acc: 0.7250\n",
      "Epoch 34, CIFAR-10 Batch 1:  Valid loss: 1.0901 Valid acc: 0.6214 Train loss: 0.9352 Train acc: 0.7250\n",
      "Epoch 34, CIFAR-10 Batch 2:  Valid loss: 1.1060 Valid acc: 0.6212 Train loss: 1.0618 Train acc: 0.5750\n",
      "Epoch 34, CIFAR-10 Batch 3:  Valid loss: 1.1060 Valid acc: 0.6226 Train loss: 0.8544 Train acc: 0.8000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Valid loss: 1.0878 Valid acc: 0.6254 Train loss: 0.8830 Train acc: 0.7250\n",
      "Epoch 34, CIFAR-10 Batch 5:  Valid loss: 1.0800 Valid acc: 0.6268 Train loss: 0.9817 Train acc: 0.7500\n",
      "Epoch 35, CIFAR-10 Batch 1:  Valid loss: 1.1057 Valid acc: 0.6100 Train loss: 0.9479 Train acc: 0.7250\n",
      "Epoch 35, CIFAR-10 Batch 2:  Valid loss: 1.1150 Valid acc: 0.6142 Train loss: 1.0876 Train acc: 0.6500\n",
      "Epoch 35, CIFAR-10 Batch 3:  Valid loss: 1.0727 Valid acc: 0.6272 Train loss: 0.8165 Train acc: 0.7500\n",
      "Epoch 35, CIFAR-10 Batch 4:  Valid loss: 1.1124 Valid acc: 0.6220 Train loss: 0.8503 Train acc: 0.7750\n",
      "Epoch 35, CIFAR-10 Batch 5:  Valid loss: 1.0790 Valid acc: 0.6206 Train loss: 0.9699 Train acc: 0.7250\n",
      "Epoch 36, CIFAR-10 Batch 1:  Valid loss: 1.0804 Valid acc: 0.6272 Train loss: 0.8642 Train acc: 0.7750\n",
      "Epoch 36, CIFAR-10 Batch 2:  Valid loss: 1.1016 Valid acc: 0.6208 Train loss: 1.0475 Train acc: 0.6000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Valid loss: 1.0740 Valid acc: 0.6350 Train loss: 0.8155 Train acc: 0.8000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Valid loss: 1.1476 Valid acc: 0.6108 Train loss: 0.9408 Train acc: 0.6500\n",
      "Epoch 36, CIFAR-10 Batch 5:  Valid loss: 1.0919 Valid acc: 0.6152 Train loss: 0.9735 Train acc: 0.7250\n",
      "Epoch 37, CIFAR-10 Batch 1:  Valid loss: 1.0592 Valid acc: 0.6318 Train loss: 0.8661 Train acc: 0.7000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Valid loss: 1.0800 Valid acc: 0.6270 Train loss: 1.0860 Train acc: 0.5500\n",
      "Epoch 37, CIFAR-10 Batch 3:  Valid loss: 1.0630 Valid acc: 0.6358 Train loss: 0.8265 Train acc: 0.8000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Valid loss: 1.1122 Valid acc: 0.6260 Train loss: 0.8821 Train acc: 0.7500\n",
      "Epoch 37, CIFAR-10 Batch 5:  Valid loss: 1.0651 Valid acc: 0.6332 Train loss: 0.9555 Train acc: 0.8000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Valid loss: 1.0692 Valid acc: 0.6374 Train loss: 0.8982 Train acc: 0.7000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Valid loss: 1.0734 Valid acc: 0.6236 Train loss: 1.0869 Train acc: 0.6250\n",
      "Epoch 38, CIFAR-10 Batch 3:  Valid loss: 1.0483 Valid acc: 0.6390 Train loss: 0.7826 Train acc: 0.8250\n",
      "Epoch 38, CIFAR-10 Batch 4:  Valid loss: 1.0985 Valid acc: 0.6198 Train loss: 0.8927 Train acc: 0.7000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Valid loss: 1.0628 Valid acc: 0.6316 Train loss: 0.9506 Train acc: 0.7750\n",
      "Epoch 39, CIFAR-10 Batch 1:  Valid loss: 1.0656 Valid acc: 0.6350 Train loss: 0.8253 Train acc: 0.7250\n",
      "Epoch 39, CIFAR-10 Batch 2:  Valid loss: 1.0832 Valid acc: 0.6318 Train loss: 1.0919 Train acc: 0.5750\n",
      "Epoch 39, CIFAR-10 Batch 3:  Valid loss: 1.0574 Valid acc: 0.6364 Train loss: 0.8290 Train acc: 0.7250\n",
      "Epoch 39, CIFAR-10 Batch 4:  Valid loss: 1.0545 Valid acc: 0.6398 Train loss: 0.8283 Train acc: 0.7500\n",
      "Epoch 39, CIFAR-10 Batch 5:  Valid loss: 1.0603 Valid acc: 0.6352 Train loss: 0.9756 Train acc: 0.7250\n",
      "Epoch 40, CIFAR-10 Batch 1:  Valid loss: 1.0425 Valid acc: 0.6448 Train loss: 0.8779 Train acc: 0.7500\n",
      "Epoch 40, CIFAR-10 Batch 2:  Valid loss: 1.0851 Valid acc: 0.6216 Train loss: 1.1028 Train acc: 0.6250\n",
      "Epoch 40, CIFAR-10 Batch 3:  Valid loss: 1.0473 Valid acc: 0.6452 Train loss: 0.7890 Train acc: 0.7750\n",
      "Epoch 40, CIFAR-10 Batch 4:  Valid loss: 1.0785 Valid acc: 0.6334 Train loss: 0.8741 Train acc: 0.7250\n",
      "Epoch 40, CIFAR-10 Batch 5:  Valid loss: 1.0596 Valid acc: 0.6294 Train loss: 0.8825 Train acc: 0.8250\n",
      "Epoch 41, CIFAR-10 Batch 1:  Valid loss: 1.0354 Valid acc: 0.6438 Train loss: 0.8305 Train acc: 0.7500\n",
      "Epoch 41, CIFAR-10 Batch 2:  Valid loss: 1.0566 Valid acc: 0.6406 Train loss: 1.0565 Train acc: 0.5750\n",
      "Epoch 41, CIFAR-10 Batch 3:  Valid loss: 1.0672 Valid acc: 0.6334 Train loss: 0.8144 Train acc: 0.7500\n",
      "Epoch 41, CIFAR-10 Batch 4:  Valid loss: 1.0590 Valid acc: 0.6392 Train loss: 0.8301 Train acc: 0.7750\n",
      "Epoch 41, CIFAR-10 Batch 5:  Valid loss: 1.0388 Valid acc: 0.6380 Train loss: 0.8950 Train acc: 0.7500\n",
      "Epoch 42, CIFAR-10 Batch 1:  Valid loss: 1.0557 Valid acc: 0.6344 Train loss: 0.8750 Train acc: 0.7500\n",
      "Epoch 42, CIFAR-10 Batch 2:  Valid loss: 1.0657 Valid acc: 0.6354 Train loss: 1.0072 Train acc: 0.6250\n",
      "Epoch 42, CIFAR-10 Batch 3:  Valid loss: 1.0383 Valid acc: 0.6442 Train loss: 0.8079 Train acc: 0.6750\n",
      "Epoch 42, CIFAR-10 Batch 4:  Valid loss: 1.0277 Valid acc: 0.6534 Train loss: 0.8263 Train acc: 0.7000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Valid loss: 1.0267 Valid acc: 0.6482 Train loss: 0.8929 Train acc: 0.7750\n",
      "Epoch 43, CIFAR-10 Batch 1:  Valid loss: 1.0296 Valid acc: 0.6418 Train loss: 0.8062 Train acc: 0.6750\n",
      "Epoch 43, CIFAR-10 Batch 2:  Valid loss: 1.0722 Valid acc: 0.6362 Train loss: 1.0609 Train acc: 0.6250\n",
      "Epoch 43, CIFAR-10 Batch 3:  Valid loss: 1.0416 Valid acc: 0.6472 Train loss: 0.7300 Train acc: 0.7750\n",
      "Epoch 43, CIFAR-10 Batch 4:  Valid loss: 1.0176 Valid acc: 0.6538 Train loss: 0.8201 Train acc: 0.7000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Valid loss: 1.0589 Valid acc: 0.6330 Train loss: 0.9217 Train acc: 0.7500\n",
      "Epoch 44, CIFAR-10 Batch 1:  Valid loss: 1.0180 Valid acc: 0.6444 Train loss: 0.8416 Train acc: 0.7250\n",
      "Epoch 44, CIFAR-10 Batch 2:  Valid loss: 1.0431 Valid acc: 0.6408 Train loss: 0.9957 Train acc: 0.6250\n",
      "Epoch 44, CIFAR-10 Batch 3:  Valid loss: 1.0093 Valid acc: 0.6562 Train loss: 0.7534 Train acc: 0.8000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Valid loss: 1.0441 Valid acc: 0.6472 Train loss: 0.8149 Train acc: 0.7250\n",
      "Epoch 44, CIFAR-10 Batch 5:  Valid loss: 1.0241 Valid acc: 0.6494 Train loss: 0.8794 Train acc: 0.7500\n",
      "Epoch 45, CIFAR-10 Batch 1:  Valid loss: 1.0402 Valid acc: 0.6352 Train loss: 0.8511 Train acc: 0.6750\n",
      "Epoch 45, CIFAR-10 Batch 2:  Valid loss: 1.0573 Valid acc: 0.6408 Train loss: 0.9470 Train acc: 0.6750\n",
      "Epoch 45, CIFAR-10 Batch 3:  Valid loss: 1.0347 Valid acc: 0.6444 Train loss: 0.7437 Train acc: 0.7750\n",
      "Epoch 45, CIFAR-10 Batch 4:  Valid loss: 1.0245 Valid acc: 0.6526 Train loss: 0.7784 Train acc: 0.7750\n",
      "Epoch 45, CIFAR-10 Batch 5:  Valid loss: 1.0354 Valid acc: 0.6348 Train loss: 0.8797 Train acc: 0.7750\n",
      "Epoch 46, CIFAR-10 Batch 1:  Valid loss: 1.0176 Valid acc: 0.6506 Train loss: 0.8164 Train acc: 0.8000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Valid loss: 1.0313 Valid acc: 0.6470 Train loss: 1.0351 Train acc: 0.6250\n",
      "Epoch 46, CIFAR-10 Batch 3:  Valid loss: 1.0094 Valid acc: 0.6540 Train loss: 0.7275 Train acc: 0.8250\n",
      "Epoch 46, CIFAR-10 Batch 4:  Valid loss: 1.0278 Valid acc: 0.6492 Train loss: 0.8106 Train acc: 0.7250\n",
      "Epoch 46, CIFAR-10 Batch 5:  Valid loss: 1.0158 Valid acc: 0.6518 Train loss: 0.8849 Train acc: 0.7500\n",
      "Epoch 47, CIFAR-10 Batch 1:  Valid loss: 1.0243 Valid acc: 0.6442 Train loss: 0.8325 Train acc: 0.7250\n",
      "Epoch 47, CIFAR-10 Batch 2:  Valid loss: 1.0426 Valid acc: 0.6364 Train loss: 0.9202 Train acc: 0.6500\n",
      "Epoch 47, CIFAR-10 Batch 3:  Valid loss: 1.0087 Valid acc: 0.6560 Train loss: 0.7148 Train acc: 0.8500\n",
      "Epoch 47, CIFAR-10 Batch 4:  Valid loss: 1.0128 Valid acc: 0.6578 Train loss: 0.7461 Train acc: 0.6750\n",
      "Epoch 47, CIFAR-10 Batch 5:  Valid loss: 0.9995 Valid acc: 0.6514 Train loss: 0.8358 Train acc: 0.7750\n",
      "Epoch 48, CIFAR-10 Batch 1:  Valid loss: 1.0467 Valid acc: 0.6398 Train loss: 0.8512 Train acc: 0.7500\n",
      "Epoch 48, CIFAR-10 Batch 2:  Valid loss: 1.0324 Valid acc: 0.6554 Train loss: 0.9168 Train acc: 0.6250\n",
      "Epoch 48, CIFAR-10 Batch 3:  Valid loss: 0.9906 Valid acc: 0.6654 Train loss: 0.7024 Train acc: 0.8500\n",
      "Epoch 48, CIFAR-10 Batch 4:  Valid loss: 0.9946 Valid acc: 0.6622 Train loss: 0.7168 Train acc: 0.7750\n",
      "Epoch 48, CIFAR-10 Batch 5:  Valid loss: 1.0167 Valid acc: 0.6502 Train loss: 0.8739 Train acc: 0.8000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Valid loss: 1.0072 Valid acc: 0.6486 Train loss: 0.8049 Train acc: 0.7750\n",
      "Epoch 49, CIFAR-10 Batch 2:  Valid loss: 0.9919 Valid acc: 0.6600 Train loss: 0.9740 Train acc: 0.5750\n",
      "Epoch 49, CIFAR-10 Batch 3:  Valid loss: 0.9991 Valid acc: 0.6548 Train loss: 0.6745 Train acc: 0.8500\n",
      "Epoch 49, CIFAR-10 Batch 4:  Valid loss: 1.0056 Valid acc: 0.6616 Train loss: 0.7307 Train acc: 0.7500\n",
      "Epoch 49, CIFAR-10 Batch 5:  Valid loss: 1.0094 Valid acc: 0.6530 Train loss: 0.8876 Train acc: 0.7750\n",
      "Epoch 50, CIFAR-10 Batch 1:  Valid loss: 0.9980 Valid acc: 0.6556 Train loss: 0.7922 Train acc: 0.7750\n",
      "Epoch 50, CIFAR-10 Batch 2:  Valid loss: 1.0238 Valid acc: 0.6388 Train loss: 0.9821 Train acc: 0.6000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Valid loss: 1.0097 Valid acc: 0.6568 Train loss: 0.7289 Train acc: 0.8500\n",
      "Epoch 50, CIFAR-10 Batch 4:  Valid loss: 1.0325 Valid acc: 0.6468 Train loss: 0.7315 Train acc: 0.8000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Valid loss: 1.0008 Valid acc: 0.6468 Train loss: 0.8749 Train acc: 0.7750\n",
      "Epoch 51, CIFAR-10 Batch 1:  Valid loss: 0.9941 Valid acc: 0.6582 Train loss: 0.7967 Train acc: 0.7500\n",
      "Epoch 51, CIFAR-10 Batch 2:  Valid loss: 0.9998 Valid acc: 0.6586 Train loss: 0.9472 Train acc: 0.6750\n",
      "Epoch 51, CIFAR-10 Batch 3:  Valid loss: 0.9905 Valid acc: 0.6624 Train loss: 0.7182 Train acc: 0.8000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Valid loss: 1.0275 Valid acc: 0.6486 Train loss: 0.7585 Train acc: 0.7500\n",
      "Epoch 51, CIFAR-10 Batch 5:  Valid loss: 1.0464 Valid acc: 0.6324 Train loss: 0.8859 Train acc: 0.7250\n",
      "Epoch 52, CIFAR-10 Batch 1:  Valid loss: 0.9761 Valid acc: 0.6664 Train loss: 0.7730 Train acc: 0.7750\n",
      "Epoch 52, CIFAR-10 Batch 2:  Valid loss: 0.9992 Valid acc: 0.6524 Train loss: 0.9271 Train acc: 0.6750\n",
      "Epoch 52, CIFAR-10 Batch 3:  Valid loss: 0.9837 Valid acc: 0.6630 Train loss: 0.6855 Train acc: 0.7750\n",
      "Epoch 52, CIFAR-10 Batch 4:  Valid loss: 0.9900 Valid acc: 0.6550 Train loss: 0.7370 Train acc: 0.7500\n",
      "Epoch 52, CIFAR-10 Batch 5:  Valid loss: 0.9995 Valid acc: 0.6560 Train loss: 0.8735 Train acc: 0.8000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Valid loss: 0.9992 Valid acc: 0.6542 Train loss: 0.8118 Train acc: 0.7750\n",
      "Epoch 53, CIFAR-10 Batch 2:  Valid loss: 0.9931 Valid acc: 0.6594 Train loss: 0.8977 Train acc: 0.6750\n",
      "Epoch 53, CIFAR-10 Batch 3:  Valid loss: 0.9721 Valid acc: 0.6716 Train loss: 0.6549 Train acc: 0.8500\n",
      "Epoch 53, CIFAR-10 Batch 4:  Valid loss: 0.9805 Valid acc: 0.6642 Train loss: 0.7310 Train acc: 0.8000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Valid loss: 0.9739 Valid acc: 0.6634 Train loss: 0.8438 Train acc: 0.8000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Valid loss: 0.9803 Valid acc: 0.6604 Train loss: 0.7615 Train acc: 0.7750\n",
      "Epoch 54, CIFAR-10 Batch 2:  Valid loss: 0.9986 Valid acc: 0.6592 Train loss: 0.8849 Train acc: 0.7000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Valid loss: 1.0115 Valid acc: 0.6574 Train loss: 0.6894 Train acc: 0.7750\n",
      "Epoch 54, CIFAR-10 Batch 4:  Valid loss: 0.9931 Valid acc: 0.6610 Train loss: 0.7735 Train acc: 0.7500\n",
      "Epoch 54, CIFAR-10 Batch 5:  Valid loss: 0.9986 Valid acc: 0.6484 Train loss: 0.8787 Train acc: 0.7750\n",
      "Epoch 55, CIFAR-10 Batch 1:  Valid loss: 0.9643 Valid acc: 0.6662 Train loss: 0.7510 Train acc: 0.7750\n",
      "Epoch 55, CIFAR-10 Batch 2:  Valid loss: 1.0042 Valid acc: 0.6590 Train loss: 0.8944 Train acc: 0.6500\n",
      "Epoch 55, CIFAR-10 Batch 3:  Valid loss: 0.9685 Valid acc: 0.6642 Train loss: 0.6708 Train acc: 0.8750\n",
      "Epoch 55, CIFAR-10 Batch 4:  Valid loss: 0.9860 Valid acc: 0.6654 Train loss: 0.7163 Train acc: 0.8000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Valid loss: 0.9806 Valid acc: 0.6584 Train loss: 0.8227 Train acc: 0.8000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Valid loss: 0.9742 Valid acc: 0.6592 Train loss: 0.7780 Train acc: 0.7750\n",
      "Epoch 56, CIFAR-10 Batch 2:  Valid loss: 1.0055 Valid acc: 0.6556 Train loss: 0.8251 Train acc: 0.7750\n",
      "Epoch 56, CIFAR-10 Batch 3:  Valid loss: 1.0470 Valid acc: 0.6410 Train loss: 0.7742 Train acc: 0.7500\n",
      "Epoch 56, CIFAR-10 Batch 4:  Valid loss: 0.9795 Valid acc: 0.6692 Train loss: 0.7132 Train acc: 0.7750\n",
      "Epoch 56, CIFAR-10 Batch 5:  Valid loss: 0.9844 Valid acc: 0.6596 Train loss: 0.8481 Train acc: 0.7750\n",
      "Epoch 57, CIFAR-10 Batch 1:  Valid loss: 0.9685 Valid acc: 0.6626 Train loss: 0.7458 Train acc: 0.7750\n",
      "Epoch 57, CIFAR-10 Batch 2:  Valid loss: 1.0067 Valid acc: 0.6522 Train loss: 0.8888 Train acc: 0.6750\n",
      "Epoch 57, CIFAR-10 Batch 3:  Valid loss: 0.9845 Valid acc: 0.6648 Train loss: 0.7130 Train acc: 0.8000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Valid loss: 0.9846 Valid acc: 0.6716 Train loss: 0.6719 Train acc: 0.7750\n",
      "Epoch 57, CIFAR-10 Batch 5:  Valid loss: 0.9912 Valid acc: 0.6550 Train loss: 0.8184 Train acc: 0.7750\n",
      "Epoch 58, CIFAR-10 Batch 1:  Valid loss: 0.9636 Valid acc: 0.6732 Train loss: 0.7330 Train acc: 0.8000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Valid loss: 0.9613 Valid acc: 0.6740 Train loss: 0.8497 Train acc: 0.6750\n",
      "Epoch 58, CIFAR-10 Batch 3:  Valid loss: 0.9541 Valid acc: 0.6722 Train loss: 0.6555 Train acc: 0.9000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Valid loss: 0.9897 Valid acc: 0.6584 Train loss: 0.7214 Train acc: 0.8000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Valid loss: 0.9713 Valid acc: 0.6620 Train loss: 0.8309 Train acc: 0.7500\n",
      "Epoch 59, CIFAR-10 Batch 1:  Valid loss: 0.9632 Valid acc: 0.6702 Train loss: 0.7331 Train acc: 0.7750\n",
      "Epoch 59, CIFAR-10 Batch 2:  Valid loss: 1.0219 Valid acc: 0.6534 Train loss: 0.8212 Train acc: 0.7250\n",
      "Epoch 59, CIFAR-10 Batch 3:  Valid loss: 1.0307 Valid acc: 0.6440 Train loss: 0.7139 Train acc: 0.8500\n",
      "Epoch 59, CIFAR-10 Batch 4:  Valid loss: 0.9576 Valid acc: 0.6766 Train loss: 0.6891 Train acc: 0.7750\n",
      "Epoch 59, CIFAR-10 Batch 5:  Valid loss: 0.9696 Valid acc: 0.6610 Train loss: 0.8059 Train acc: 0.7500\n",
      "Epoch 60, CIFAR-10 Batch 1:  Valid loss: 0.9640 Valid acc: 0.6674 Train loss: 0.7419 Train acc: 0.7750\n",
      "Epoch 60, CIFAR-10 Batch 2:  Valid loss: 0.9684 Valid acc: 0.6690 Train loss: 0.8600 Train acc: 0.7750\n",
      "Epoch 60, CIFAR-10 Batch 3:  Valid loss: 1.0202 Valid acc: 0.6474 Train loss: 0.6971 Train acc: 0.8750\n",
      "Epoch 60, CIFAR-10 Batch 4:  Valid loss: 0.9701 Valid acc: 0.6656 Train loss: 0.6838 Train acc: 0.7750\n",
      "Epoch 60, CIFAR-10 Batch 5:  Valid loss: 0.9631 Valid acc: 0.6616 Train loss: 0.7825 Train acc: 0.7750\n",
      "Epoch 61, CIFAR-10 Batch 1:  Valid loss: 0.9671 Valid acc: 0.6724 Train loss: 0.7671 Train acc: 0.7750\n",
      "Epoch 61, CIFAR-10 Batch 2:  Valid loss: 0.9971 Valid acc: 0.6552 Train loss: 0.8726 Train acc: 0.7250\n",
      "Epoch 61, CIFAR-10 Batch 3:  Valid loss: 0.9542 Valid acc: 0.6782 Train loss: 0.6399 Train acc: 0.8750\n",
      "Epoch 61, CIFAR-10 Batch 4:  Valid loss: 0.9850 Valid acc: 0.6664 Train loss: 0.7084 Train acc: 0.8000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Valid loss: 0.9688 Valid acc: 0.6670 Train loss: 0.7887 Train acc: 0.7750\n",
      "Epoch 62, CIFAR-10 Batch 1:  Valid loss: 0.9592 Valid acc: 0.6726 Train loss: 0.7884 Train acc: 0.7250\n",
      "Epoch 62, CIFAR-10 Batch 2:  Valid loss: 0.9801 Valid acc: 0.6654 Train loss: 0.8937 Train acc: 0.7500\n",
      "Epoch 62, CIFAR-10 Batch 3:  Valid loss: 1.0075 Valid acc: 0.6560 Train loss: 0.7241 Train acc: 0.8000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Valid loss: 0.9851 Valid acc: 0.6692 Train loss: 0.6731 Train acc: 0.7500\n",
      "Epoch 62, CIFAR-10 Batch 5:  Valid loss: 0.9882 Valid acc: 0.6582 Train loss: 0.8455 Train acc: 0.7750\n",
      "Epoch 63, CIFAR-10 Batch 1:  Valid loss: 0.9479 Valid acc: 0.6710 Train loss: 0.7440 Train acc: 0.7250\n",
      "Epoch 63, CIFAR-10 Batch 2:  Valid loss: 0.9765 Valid acc: 0.6734 Train loss: 0.8710 Train acc: 0.7250\n",
      "Epoch 63, CIFAR-10 Batch 3:  Valid loss: 0.9752 Valid acc: 0.6642 Train loss: 0.6495 Train acc: 0.8750\n",
      "Epoch 63, CIFAR-10 Batch 4:  Valid loss: 0.9539 Valid acc: 0.6832 Train loss: 0.6331 Train acc: 0.7500\n",
      "Epoch 63, CIFAR-10 Batch 5:  Valid loss: 0.9571 Valid acc: 0.6656 Train loss: 0.7623 Train acc: 0.8000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Valid loss: 0.9416 Valid acc: 0.6744 Train loss: 0.7635 Train acc: 0.7750\n",
      "Epoch 64, CIFAR-10 Batch 2:  Valid loss: 0.9649 Valid acc: 0.6670 Train loss: 0.7941 Train acc: 0.7250\n",
      "Epoch 64, CIFAR-10 Batch 3:  Valid loss: 0.9915 Valid acc: 0.6622 Train loss: 0.6724 Train acc: 0.8500\n",
      "Epoch 64, CIFAR-10 Batch 4:  Valid loss: 0.9922 Valid acc: 0.6584 Train loss: 0.6642 Train acc: 0.8000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Valid loss: 0.9394 Valid acc: 0.6772 Train loss: 0.7771 Train acc: 0.8250\n",
      "Epoch 65, CIFAR-10 Batch 1:  Valid loss: 0.9370 Valid acc: 0.6836 Train loss: 0.7451 Train acc: 0.7500\n",
      "Epoch 65, CIFAR-10 Batch 2:  Valid loss: 0.9695 Valid acc: 0.6608 Train loss: 0.7593 Train acc: 0.7500\n",
      "Epoch 65, CIFAR-10 Batch 3:  Valid loss: 0.9672 Valid acc: 0.6692 Train loss: 0.6701 Train acc: 0.9000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Valid loss: 0.9753 Valid acc: 0.6706 Train loss: 0.6896 Train acc: 0.8000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Valid loss: 0.9551 Valid acc: 0.6684 Train loss: 0.7937 Train acc: 0.8000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Valid loss: 0.9462 Valid acc: 0.6736 Train loss: 0.7788 Train acc: 0.7500\n",
      "Epoch 66, CIFAR-10 Batch 2:  Valid loss: 0.9453 Valid acc: 0.6782 Train loss: 0.8206 Train acc: 0.8000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Valid loss: 0.9643 Valid acc: 0.6736 Train loss: 0.6293 Train acc: 0.9000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Valid loss: 0.9475 Valid acc: 0.6800 Train loss: 0.6621 Train acc: 0.8250\n",
      "Epoch 66, CIFAR-10 Batch 5:  Valid loss: 0.9395 Valid acc: 0.6748 Train loss: 0.7757 Train acc: 0.7750\n",
      "Epoch 67, CIFAR-10 Batch 1:  Valid loss: 0.9411 Valid acc: 0.6778 Train loss: 0.7313 Train acc: 0.7750\n",
      "Epoch 67, CIFAR-10 Batch 2:  Valid loss: 0.9585 Valid acc: 0.6668 Train loss: 0.7455 Train acc: 0.7750\n",
      "Epoch 67, CIFAR-10 Batch 3:  Valid loss: 0.9427 Valid acc: 0.6786 Train loss: 0.5804 Train acc: 0.9250\n",
      "Epoch 67, CIFAR-10 Batch 4:  Valid loss: 0.9646 Valid acc: 0.6690 Train loss: 0.6917 Train acc: 0.7750\n",
      "Epoch 67, CIFAR-10 Batch 5:  Valid loss: 0.9406 Valid acc: 0.6818 Train loss: 0.7888 Train acc: 0.8250\n",
      "Epoch 68, CIFAR-10 Batch 1:  Valid loss: 0.9313 Valid acc: 0.6854 Train loss: 0.7622 Train acc: 0.7500\n",
      "Epoch 68, CIFAR-10 Batch 2:  Valid loss: 0.9374 Valid acc: 0.6768 Train loss: 0.7570 Train acc: 0.7250\n",
      "Epoch 68, CIFAR-10 Batch 3:  Valid loss: 0.9614 Valid acc: 0.6744 Train loss: 0.5772 Train acc: 0.9000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Valid loss: 0.9605 Valid acc: 0.6792 Train loss: 0.6482 Train acc: 0.8250\n",
      "Epoch 68, CIFAR-10 Batch 5:  Valid loss: 0.9542 Valid acc: 0.6722 Train loss: 0.8223 Train acc: 0.8250\n",
      "Epoch 69, CIFAR-10 Batch 1:  Valid loss: 0.9319 Valid acc: 0.6826 Train loss: 0.7209 Train acc: 0.7750\n",
      "Epoch 69, CIFAR-10 Batch 2:  Valid loss: 0.9433 Valid acc: 0.6814 Train loss: 0.7550 Train acc: 0.7500\n",
      "Epoch 69, CIFAR-10 Batch 3:  Valid loss: 0.9556 Valid acc: 0.6698 Train loss: 0.6421 Train acc: 0.9000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Valid loss: 0.9602 Valid acc: 0.6722 Train loss: 0.6577 Train acc: 0.8250\n",
      "Epoch 69, CIFAR-10 Batch 5:  Valid loss: 0.9573 Valid acc: 0.6694 Train loss: 0.7412 Train acc: 0.8750\n",
      "Epoch 70, CIFAR-10 Batch 1:  Valid loss: 0.9458 Valid acc: 0.6736 Train loss: 0.7656 Train acc: 0.7750\n",
      "Epoch 70, CIFAR-10 Batch 2:  Valid loss: 0.9622 Valid acc: 0.6656 Train loss: 0.7469 Train acc: 0.7500\n",
      "Epoch 70, CIFAR-10 Batch 3:  Valid loss: 0.9241 Valid acc: 0.6844 Train loss: 0.5496 Train acc: 0.9250\n",
      "Epoch 70, CIFAR-10 Batch 4:  Valid loss: 0.9437 Valid acc: 0.6844 Train loss: 0.6529 Train acc: 0.8250\n",
      "Epoch 70, CIFAR-10 Batch 5:  Valid loss: 0.9362 Valid acc: 0.6778 Train loss: 0.7516 Train acc: 0.8250\n",
      "Epoch 71, CIFAR-10 Batch 1:  Valid loss: 0.9400 Valid acc: 0.6786 Train loss: 0.7499 Train acc: 0.7500\n",
      "Epoch 71, CIFAR-10 Batch 2:  Valid loss: 0.9589 Valid acc: 0.6652 Train loss: 0.7501 Train acc: 0.7000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Valid loss: 0.9458 Valid acc: 0.6782 Train loss: 0.5948 Train acc: 0.9000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Valid loss: 0.9326 Valid acc: 0.6846 Train loss: 0.6278 Train acc: 0.8000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Valid loss: 0.9324 Valid acc: 0.6768 Train loss: 0.7783 Train acc: 0.8000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Valid loss: 0.9190 Valid acc: 0.6886 Train loss: 0.6710 Train acc: 0.8250\n",
      "Epoch 72, CIFAR-10 Batch 2:  Valid loss: 0.9605 Valid acc: 0.6744 Train loss: 0.7573 Train acc: 0.7250\n",
      "Epoch 72, CIFAR-10 Batch 3:  Valid loss: 0.9430 Valid acc: 0.6766 Train loss: 0.6101 Train acc: 0.8750\n",
      "Epoch 72, CIFAR-10 Batch 4:  Valid loss: 0.9363 Valid acc: 0.6828 Train loss: 0.6055 Train acc: 0.8250\n",
      "Epoch 72, CIFAR-10 Batch 5:  Valid loss: 0.9312 Valid acc: 0.6746 Train loss: 0.7855 Train acc: 0.8250\n",
      "Epoch 73, CIFAR-10 Batch 1:  Valid loss: 0.9279 Valid acc: 0.6878 Train loss: 0.6819 Train acc: 0.8250\n",
      "Epoch 73, CIFAR-10 Batch 2:  Valid loss: 0.9729 Valid acc: 0.6656 Train loss: 0.8365 Train acc: 0.7000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Valid loss: 0.9462 Valid acc: 0.6720 Train loss: 0.6107 Train acc: 0.8250\n",
      "Epoch 73, CIFAR-10 Batch 4:  Valid loss: 0.9254 Valid acc: 0.6822 Train loss: 0.6022 Train acc: 0.8750\n",
      "Epoch 73, CIFAR-10 Batch 5:  Valid loss: 0.9510 Valid acc: 0.6726 Train loss: 0.8188 Train acc: 0.8000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Valid loss: 0.9120 Valid acc: 0.6904 Train loss: 0.7258 Train acc: 0.7500\n",
      "Epoch 74, CIFAR-10 Batch 2:  Valid loss: 0.9389 Valid acc: 0.6786 Train loss: 0.7388 Train acc: 0.7250\n",
      "Epoch 74, CIFAR-10 Batch 3:  Valid loss: 0.9442 Valid acc: 0.6716 Train loss: 0.6270 Train acc: 0.8250\n",
      "Epoch 74, CIFAR-10 Batch 4:  Valid loss: 0.9416 Valid acc: 0.6764 Train loss: 0.6443 Train acc: 0.9000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Valid loss: 0.9353 Valid acc: 0.6750 Train loss: 0.7780 Train acc: 0.8250\n",
      "Epoch 75, CIFAR-10 Batch 1:  Valid loss: 0.9461 Valid acc: 0.6666 Train loss: 0.7185 Train acc: 0.7750\n",
      "Epoch 75, CIFAR-10 Batch 2:  Valid loss: 0.9845 Valid acc: 0.6602 Train loss: 0.7622 Train acc: 0.7250\n",
      "Epoch 75, CIFAR-10 Batch 3:  Valid loss: 0.9239 Valid acc: 0.6804 Train loss: 0.6039 Train acc: 0.8750\n",
      "Epoch 75, CIFAR-10 Batch 4:  Valid loss: 0.9401 Valid acc: 0.6772 Train loss: 0.6156 Train acc: 0.8250\n",
      "Epoch 75, CIFAR-10 Batch 5:  Valid loss: 0.9321 Valid acc: 0.6772 Train loss: 0.7594 Train acc: 0.8500\n",
      "Epoch 76, CIFAR-10 Batch 1:  Valid loss: 0.9387 Valid acc: 0.6736 Train loss: 0.7097 Train acc: 0.8000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Valid loss: 0.9737 Valid acc: 0.6706 Train loss: 0.7411 Train acc: 0.7250\n",
      "Epoch 76, CIFAR-10 Batch 3:  Valid loss: 0.9391 Valid acc: 0.6782 Train loss: 0.6359 Train acc: 0.8000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Valid loss: 0.9119 Valid acc: 0.6896 Train loss: 0.5873 Train acc: 0.8250\n",
      "Epoch 76, CIFAR-10 Batch 5:  Valid loss: 0.9291 Valid acc: 0.6810 Train loss: 0.7205 Train acc: 0.8750\n",
      "Epoch 77, CIFAR-10 Batch 1:  Valid loss: 0.9289 Valid acc: 0.6786 Train loss: 0.6634 Train acc: 0.8000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Valid loss: 0.9640 Valid acc: 0.6694 Train loss: 0.7742 Train acc: 0.7500\n",
      "Epoch 77, CIFAR-10 Batch 3:  Valid loss: 0.9389 Valid acc: 0.6744 Train loss: 0.5670 Train acc: 0.9000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Valid loss: 0.9185 Valid acc: 0.6952 Train loss: 0.6213 Train acc: 0.8000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Valid loss: 0.9278 Valid acc: 0.6818 Train loss: 0.7209 Train acc: 0.8000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Valid loss: 0.9023 Valid acc: 0.6884 Train loss: 0.6972 Train acc: 0.8000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Valid loss: 0.9530 Valid acc: 0.6670 Train loss: 0.7709 Train acc: 0.6750\n",
      "Epoch 78, CIFAR-10 Batch 3:  Valid loss: 0.9420 Valid acc: 0.6838 Train loss: 0.5818 Train acc: 0.9250\n",
      "Epoch 78, CIFAR-10 Batch 4:  Valid loss: 0.9162 Valid acc: 0.6900 Train loss: 0.6008 Train acc: 0.8500\n",
      "Epoch 78, CIFAR-10 Batch 5:  Valid loss: 0.9365 Valid acc: 0.6682 Train loss: 0.7638 Train acc: 0.7750\n",
      "Epoch 79, CIFAR-10 Batch 1:  Valid loss: 0.9273 Valid acc: 0.6818 Train loss: 0.7267 Train acc: 0.7750\n",
      "Epoch 79, CIFAR-10 Batch 2:  Valid loss: 0.9416 Valid acc: 0.6820 Train loss: 0.7591 Train acc: 0.6750\n",
      "Epoch 79, CIFAR-10 Batch 3:  Valid loss: 0.9274 Valid acc: 0.6898 Train loss: 0.5702 Train acc: 0.9000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Valid loss: 0.9054 Valid acc: 0.6898 Train loss: 0.5857 Train acc: 0.8500\n",
      "Epoch 79, CIFAR-10 Batch 5:  Valid loss: 0.9253 Valid acc: 0.6796 Train loss: 0.7465 Train acc: 0.7750\n",
      "Epoch 80, CIFAR-10 Batch 1:  Valid loss: 0.8979 Valid acc: 0.6926 Train loss: 0.7079 Train acc: 0.7750\n",
      "Epoch 80, CIFAR-10 Batch 2:  Valid loss: 0.9205 Valid acc: 0.6818 Train loss: 0.7421 Train acc: 0.7500\n",
      "Epoch 80, CIFAR-10 Batch 3:  Valid loss: 0.9024 Valid acc: 0.6926 Train loss: 0.5487 Train acc: 0.8750\n",
      "Epoch 80, CIFAR-10 Batch 4:  Valid loss: 0.9162 Valid acc: 0.6904 Train loss: 0.5856 Train acc: 0.8500\n",
      "Epoch 80, CIFAR-10 Batch 5:  Valid loss: 0.9394 Valid acc: 0.6718 Train loss: 0.7306 Train acc: 0.8250\n",
      "Epoch 81, CIFAR-10 Batch 1:  Valid loss: 0.9017 Valid acc: 0.6884 Train loss: 0.6836 Train acc: 0.7750\n",
      "Epoch 81, CIFAR-10 Batch 2:  Valid loss: 0.9232 Valid acc: 0.6842 Train loss: 0.6914 Train acc: 0.7250\n",
      "Epoch 81, CIFAR-10 Batch 3:  Valid loss: 0.9255 Valid acc: 0.6790 Train loss: 0.5402 Train acc: 0.9000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Valid loss: 0.9295 Valid acc: 0.6824 Train loss: 0.5957 Train acc: 0.7750\n",
      "Epoch 81, CIFAR-10 Batch 5:  Valid loss: 0.9204 Valid acc: 0.6892 Train loss: 0.7572 Train acc: 0.7750\n",
      "Epoch 82, CIFAR-10 Batch 1:  Valid loss: 0.9292 Valid acc: 0.6812 Train loss: 0.7342 Train acc: 0.7250\n",
      "Epoch 82, CIFAR-10 Batch 2:  Valid loss: 0.9372 Valid acc: 0.6874 Train loss: 0.6810 Train acc: 0.7250\n",
      "Epoch 82, CIFAR-10 Batch 3:  Valid loss: 0.9497 Valid acc: 0.6776 Train loss: 0.5895 Train acc: 0.8500\n",
      "Epoch 82, CIFAR-10 Batch 4:  Valid loss: 0.9182 Valid acc: 0.6880 Train loss: 0.5903 Train acc: 0.8000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Valid loss: 0.9206 Valid acc: 0.6870 Train loss: 0.7365 Train acc: 0.8000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Valid loss: 0.9083 Valid acc: 0.6832 Train loss: 0.6968 Train acc: 0.7250\n",
      "Epoch 83, CIFAR-10 Batch 2:  Valid loss: 0.9263 Valid acc: 0.6790 Train loss: 0.7351 Train acc: 0.7250\n",
      "Epoch 83, CIFAR-10 Batch 3:  Valid loss: 0.9807 Valid acc: 0.6580 Train loss: 0.5692 Train acc: 0.9250\n",
      "Epoch 83, CIFAR-10 Batch 4:  Valid loss: 0.9334 Valid acc: 0.6860 Train loss: 0.6175 Train acc: 0.8750\n",
      "Epoch 83, CIFAR-10 Batch 5:  Valid loss: 0.9327 Valid acc: 0.6760 Train loss: 0.7112 Train acc: 0.8250\n",
      "Epoch 84, CIFAR-10 Batch 1:  Valid loss: 0.9129 Valid acc: 0.6868 Train loss: 0.6730 Train acc: 0.7750\n",
      "Epoch 84, CIFAR-10 Batch 2:  Valid loss: 0.9252 Valid acc: 0.6868 Train loss: 0.7079 Train acc: 0.7500\n",
      "Epoch 84, CIFAR-10 Batch 3:  Valid loss: 0.9232 Valid acc: 0.6828 Train loss: 0.5233 Train acc: 0.9000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Valid loss: 0.9171 Valid acc: 0.6914 Train loss: 0.5954 Train acc: 0.8750\n",
      "Epoch 84, CIFAR-10 Batch 5:  Valid loss: 0.9149 Valid acc: 0.6840 Train loss: 0.6722 Train acc: 0.8250\n",
      "Epoch 85, CIFAR-10 Batch 1:  Valid loss: 0.8971 Valid acc: 0.7018 Train loss: 0.6450 Train acc: 0.7750\n",
      "Epoch 85, CIFAR-10 Batch 2:  Valid loss: 0.9073 Valid acc: 0.6896 Train loss: 0.7070 Train acc: 0.7250\n",
      "Epoch 85, CIFAR-10 Batch 3:  Valid loss: 0.9189 Valid acc: 0.6876 Train loss: 0.5294 Train acc: 0.8750\n",
      "Epoch 85, CIFAR-10 Batch 4:  Valid loss: 0.9057 Valid acc: 0.6886 Train loss: 0.5482 Train acc: 0.8750\n",
      "Epoch 85, CIFAR-10 Batch 5:  Valid loss: 0.9454 Valid acc: 0.6744 Train loss: 0.7011 Train acc: 0.8000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Valid loss: 0.8889 Valid acc: 0.6924 Train loss: 0.6689 Train acc: 0.7500\n",
      "Epoch 86, CIFAR-10 Batch 2:  Valid loss: 0.9374 Valid acc: 0.6882 Train loss: 0.6868 Train acc: 0.7500\n",
      "Epoch 86, CIFAR-10 Batch 3:  Valid loss: 0.8915 Valid acc: 0.6938 Train loss: 0.5094 Train acc: 0.9500\n",
      "Epoch 86, CIFAR-10 Batch 4:  Valid loss: 0.9316 Valid acc: 0.6798 Train loss: 0.5693 Train acc: 0.8250\n",
      "Epoch 86, CIFAR-10 Batch 5:  Valid loss: 0.9368 Valid acc: 0.6764 Train loss: 0.6630 Train acc: 0.8000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Valid loss: 0.9211 Valid acc: 0.6840 Train loss: 0.6643 Train acc: 0.7250\n",
      "Epoch 87, CIFAR-10 Batch 2:  Valid loss: 0.9486 Valid acc: 0.6802 Train loss: 0.6718 Train acc: 0.7250\n",
      "Epoch 87, CIFAR-10 Batch 3:  Valid loss: 0.9119 Valid acc: 0.6830 Train loss: 0.5244 Train acc: 0.8500\n",
      "Epoch 87, CIFAR-10 Batch 4:  Valid loss: 0.9117 Valid acc: 0.6904 Train loss: 0.5753 Train acc: 0.8250\n",
      "Epoch 87, CIFAR-10 Batch 5:  Valid loss: 0.9121 Valid acc: 0.6866 Train loss: 0.6690 Train acc: 0.8000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Valid loss: 0.8886 Valid acc: 0.7060 Train loss: 0.6693 Train acc: 0.7750\n",
      "Epoch 88, CIFAR-10 Batch 2:  Valid loss: 0.9317 Valid acc: 0.6828 Train loss: 0.6227 Train acc: 0.7750\n",
      "Epoch 88, CIFAR-10 Batch 3:  Valid loss: 0.9146 Valid acc: 0.6872 Train loss: 0.5054 Train acc: 0.9500\n",
      "Epoch 88, CIFAR-10 Batch 4:  Valid loss: 0.9105 Valid acc: 0.6954 Train loss: 0.5874 Train acc: 0.8250\n",
      "Epoch 88, CIFAR-10 Batch 5:  Valid loss: 0.9159 Valid acc: 0.6878 Train loss: 0.6605 Train acc: 0.8250\n",
      "Epoch 89, CIFAR-10 Batch 1:  Valid loss: 0.8807 Valid acc: 0.7022 Train loss: 0.6181 Train acc: 0.8000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Valid loss: 0.9219 Valid acc: 0.6894 Train loss: 0.6622 Train acc: 0.8000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Valid loss: 0.9184 Valid acc: 0.6876 Train loss: 0.5025 Train acc: 0.9250\n",
      "Epoch 89, CIFAR-10 Batch 4:  Valid loss: 0.9238 Valid acc: 0.6902 Train loss: 0.5810 Train acc: 0.8250\n",
      "Epoch 89, CIFAR-10 Batch 5:  Valid loss: 0.8888 Valid acc: 0.6964 Train loss: 0.6206 Train acc: 0.8750\n",
      "Epoch 90, CIFAR-10 Batch 1:  Valid loss: 0.8764 Valid acc: 0.6996 Train loss: 0.6028 Train acc: 0.7750\n",
      "Epoch 90, CIFAR-10 Batch 2:  Valid loss: 0.9521 Valid acc: 0.6714 Train loss: 0.7085 Train acc: 0.7500\n",
      "Epoch 90, CIFAR-10 Batch 3:  Valid loss: 0.9184 Valid acc: 0.6830 Train loss: 0.5411 Train acc: 0.8500\n",
      "Epoch 90, CIFAR-10 Batch 4:  Valid loss: 0.8964 Valid acc: 0.6952 Train loss: 0.5469 Train acc: 0.9000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Valid loss: 0.8960 Valid acc: 0.6978 Train loss: 0.6625 Train acc: 0.8500\n",
      "Epoch 91, CIFAR-10 Batch 1:  Valid loss: 0.8838 Valid acc: 0.6918 Train loss: 0.6743 Train acc: 0.7750\n",
      "Epoch 91, CIFAR-10 Batch 2:  Valid loss: 0.9168 Valid acc: 0.6828 Train loss: 0.6373 Train acc: 0.8250\n",
      "Epoch 91, CIFAR-10 Batch 3:  Valid loss: 0.8953 Valid acc: 0.6958 Train loss: 0.5174 Train acc: 0.9000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Valid loss: 0.8965 Valid acc: 0.6946 Train loss: 0.5634 Train acc: 0.8500\n",
      "Epoch 91, CIFAR-10 Batch 5:  Valid loss: 0.9121 Valid acc: 0.6838 Train loss: 0.6389 Train acc: 0.8250\n",
      "Epoch 92, CIFAR-10 Batch 1:  Valid loss: 0.8942 Valid acc: 0.6842 Train loss: 0.7163 Train acc: 0.7500\n",
      "Epoch 92, CIFAR-10 Batch 2:  Valid loss: 0.9133 Valid acc: 0.6896 Train loss: 0.6216 Train acc: 0.8000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Valid loss: 0.8805 Valid acc: 0.6980 Train loss: 0.4581 Train acc: 0.9500\n",
      "Epoch 92, CIFAR-10 Batch 4:  Valid loss: 0.8811 Valid acc: 0.7050 Train loss: 0.5262 Train acc: 0.8750\n",
      "Epoch 92, CIFAR-10 Batch 5:  Valid loss: 0.9055 Valid acc: 0.6886 Train loss: 0.6342 Train acc: 0.8250\n",
      "Epoch 93, CIFAR-10 Batch 1:  Valid loss: 0.8791 Valid acc: 0.7026 Train loss: 0.6533 Train acc: 0.7750\n",
      "Epoch 93, CIFAR-10 Batch 2:  Valid loss: 0.9236 Valid acc: 0.6866 Train loss: 0.6800 Train acc: 0.7500\n",
      "Epoch 93, CIFAR-10 Batch 3:  Valid loss: 0.9037 Valid acc: 0.6930 Train loss: 0.4911 Train acc: 0.9500\n",
      "Epoch 93, CIFAR-10 Batch 4:  Valid loss: 0.8912 Valid acc: 0.7024 Train loss: 0.5279 Train acc: 0.9250\n",
      "Epoch 93, CIFAR-10 Batch 5:  Valid loss: 0.9212 Valid acc: 0.6880 Train loss: 0.6604 Train acc: 0.8500\n",
      "Epoch 94, CIFAR-10 Batch 1:  Valid loss: 0.8701 Valid acc: 0.7086 Train loss: 0.6325 Train acc: 0.7750\n",
      "Epoch 94, CIFAR-10 Batch 2:  Valid loss: 0.8922 Valid acc: 0.7010 Train loss: 0.6059 Train acc: 0.8000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Valid loss: 0.8942 Valid acc: 0.6948 Train loss: 0.4916 Train acc: 0.9500\n",
      "Epoch 94, CIFAR-10 Batch 4:  Valid loss: 0.8920 Valid acc: 0.6942 Train loss: 0.5360 Train acc: 0.8750\n",
      "Epoch 94, CIFAR-10 Batch 5:  Valid loss: 0.9049 Valid acc: 0.6906 Train loss: 0.6489 Train acc: 0.8750\n",
      "Epoch 95, CIFAR-10 Batch 1:  Valid loss: 0.8940 Valid acc: 0.7000 Train loss: 0.6450 Train acc: 0.8000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Valid loss: 0.9043 Valid acc: 0.6880 Train loss: 0.6400 Train acc: 0.8250\n",
      "Epoch 95, CIFAR-10 Batch 3:  Valid loss: 0.9238 Valid acc: 0.6896 Train loss: 0.5463 Train acc: 0.9250\n",
      "Epoch 95, CIFAR-10 Batch 4:  Valid loss: 0.8960 Valid acc: 0.7074 Train loss: 0.5450 Train acc: 0.8500\n",
      "Epoch 95, CIFAR-10 Batch 5:  Valid loss: 0.8965 Valid acc: 0.6926 Train loss: 0.6080 Train acc: 0.8250\n",
      "Epoch 96, CIFAR-10 Batch 1:  Valid loss: 0.8723 Valid acc: 0.7056 Train loss: 0.6269 Train acc: 0.7750\n",
      "Epoch 96, CIFAR-10 Batch 2:  Valid loss: 0.9393 Valid acc: 0.6814 Train loss: 0.6688 Train acc: 0.7250\n",
      "Epoch 96, CIFAR-10 Batch 3:  Valid loss: 0.9249 Valid acc: 0.6810 Train loss: 0.5322 Train acc: 0.9000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Valid loss: 0.8975 Valid acc: 0.6962 Train loss: 0.4909 Train acc: 0.9000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Valid loss: 0.9063 Valid acc: 0.6920 Train loss: 0.6002 Train acc: 0.8750\n",
      "Epoch 97, CIFAR-10 Batch 1:  Valid loss: 0.9013 Valid acc: 0.6926 Train loss: 0.6644 Train acc: 0.8000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Valid loss: 0.9020 Valid acc: 0.6958 Train loss: 0.6351 Train acc: 0.7750\n",
      "Epoch 97, CIFAR-10 Batch 3:  Valid loss: 0.8963 Valid acc: 0.6878 Train loss: 0.5361 Train acc: 0.8500\n",
      "Epoch 97, CIFAR-10 Batch 4:  Valid loss: 0.8843 Valid acc: 0.6984 Train loss: 0.5264 Train acc: 0.8250\n",
      "Epoch 97, CIFAR-10 Batch 5:  Valid loss: 0.8780 Valid acc: 0.6966 Train loss: 0.5767 Train acc: 0.8750\n",
      "Epoch 98, CIFAR-10 Batch 1:  Valid loss: 0.8708 Valid acc: 0.7064 Train loss: 0.6328 Train acc: 0.7250\n",
      "Epoch 98, CIFAR-10 Batch 2:  Valid loss: 0.9038 Valid acc: 0.6982 Train loss: 0.6930 Train acc: 0.7250\n",
      "Epoch 98, CIFAR-10 Batch 3:  Valid loss: 0.9511 Valid acc: 0.6720 Train loss: 0.5464 Train acc: 0.9250\n",
      "Epoch 98, CIFAR-10 Batch 4:  Valid loss: 0.8833 Valid acc: 0.6970 Train loss: 0.5215 Train acc: 0.8500\n",
      "Epoch 98, CIFAR-10 Batch 5:  Valid loss: 0.9003 Valid acc: 0.6904 Train loss: 0.6578 Train acc: 0.8500\n",
      "Epoch 99, CIFAR-10 Batch 1:  Valid loss: 0.9000 Valid acc: 0.6926 Train loss: 0.6891 Train acc: 0.7500\n",
      "Epoch 99, CIFAR-10 Batch 2:  Valid loss: 0.8900 Valid acc: 0.6960 Train loss: 0.6386 Train acc: 0.8000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Valid loss: 0.8976 Valid acc: 0.6870 Train loss: 0.5169 Train acc: 0.9250\n",
      "Epoch 99, CIFAR-10 Batch 4:  Valid loss: 0.8858 Valid acc: 0.7006 Train loss: 0.5091 Train acc: 0.9000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Valid loss: 0.9101 Valid acc: 0.6878 Train loss: 0.6257 Train acc: 0.8250\n",
      "Epoch 100, CIFAR-10 Batch 1:  Valid loss: 0.8767 Valid acc: 0.7042 Train loss: 0.6109 Train acc: 0.8000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Valid loss: 0.8882 Valid acc: 0.7010 Train loss: 0.6219 Train acc: 0.7500\n",
      "Epoch 100, CIFAR-10 Batch 3:  Valid loss: 0.9254 Valid acc: 0.6806 Train loss: 0.5453 Train acc: 0.9250\n",
      "Epoch 100, CIFAR-10 Batch 4:  Valid loss: 0.8756 Valid acc: 0.7040 Train loss: 0.5213 Train acc: 0.8750\n",
      "Epoch 100, CIFAR-10 Batch 5:  Valid loss: 0.8831 Valid acc: 0.6980 Train loss: 0.5818 Train acc: 0.8500\n",
      "Epoch 101, CIFAR-10 Batch 1:  Valid loss: 0.8607 Valid acc: 0.7048 Train loss: 0.6242 Train acc: 0.7750\n",
      "Epoch 101, CIFAR-10 Batch 2:  Valid loss: 0.9058 Valid acc: 0.6900 Train loss: 0.6305 Train acc: 0.7750\n",
      "Epoch 101, CIFAR-10 Batch 3:  Valid loss: 0.8998 Valid acc: 0.6886 Train loss: 0.5134 Train acc: 0.9250\n",
      "Epoch 101, CIFAR-10 Batch 4:  Valid loss: 0.8884 Valid acc: 0.6916 Train loss: 0.5183 Train acc: 0.8750\n",
      "Epoch 101, CIFAR-10 Batch 5:  Valid loss: 0.8884 Valid acc: 0.6974 Train loss: 0.5590 Train acc: 0.8750\n",
      "Epoch 102, CIFAR-10 Batch 1:  Valid loss: 0.8968 Valid acc: 0.6906 Train loss: 0.6766 Train acc: 0.8000\n",
      "Epoch 102, CIFAR-10 Batch 2:  Valid loss: 0.9001 Valid acc: 0.6940 Train loss: 0.6062 Train acc: 0.7500\n",
      "Epoch 102, CIFAR-10 Batch 3:  Valid loss: 0.9001 Valid acc: 0.6986 Train loss: 0.5004 Train acc: 0.9250\n",
      "Epoch 102, CIFAR-10 Batch 4:  Valid loss: 0.8850 Valid acc: 0.7044 Train loss: 0.4865 Train acc: 0.9250\n",
      "Epoch 102, CIFAR-10 Batch 5:  Valid loss: 0.9273 Valid acc: 0.6780 Train loss: 0.6294 Train acc: 0.8250\n",
      "Epoch 103, CIFAR-10 Batch 1:  Valid loss: 0.8832 Valid acc: 0.6942 Train loss: 0.6264 Train acc: 0.7750\n",
      "Epoch 103, CIFAR-10 Batch 2:  Valid loss: 0.9138 Valid acc: 0.6924 Train loss: 0.6272 Train acc: 0.8000\n",
      "Epoch 103, CIFAR-10 Batch 3:  Valid loss: 0.8902 Valid acc: 0.6984 Train loss: 0.4778 Train acc: 0.9750\n",
      "Epoch 103, CIFAR-10 Batch 4:  Valid loss: 0.8718 Valid acc: 0.7038 Train loss: 0.4956 Train acc: 0.8750\n",
      "Epoch 103, CIFAR-10 Batch 5:  Valid loss: 0.8818 Valid acc: 0.6962 Train loss: 0.5863 Train acc: 0.8250\n",
      "Epoch 104, CIFAR-10 Batch 1:  Valid loss: 0.8808 Valid acc: 0.7010 Train loss: 0.6691 Train acc: 0.7500\n",
      "Epoch 104, CIFAR-10 Batch 2:  Valid loss: 0.8725 Valid acc: 0.7016 Train loss: 0.5911 Train acc: 0.8000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Valid loss: 0.8652 Valid acc: 0.7046 Train loss: 0.4502 Train acc: 0.9750\n",
      "Epoch 104, CIFAR-10 Batch 4:  Valid loss: 0.8795 Valid acc: 0.7012 Train loss: 0.5317 Train acc: 0.8750\n",
      "Epoch 104, CIFAR-10 Batch 5:  Valid loss: 0.8718 Valid acc: 0.7002 Train loss: 0.5745 Train acc: 0.9000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Valid loss: 0.8699 Valid acc: 0.7000 Train loss: 0.6244 Train acc: 0.7500\n",
      "Epoch 105, CIFAR-10 Batch 2:  Valid loss: 0.9099 Valid acc: 0.6874 Train loss: 0.6378 Train acc: 0.7750\n",
      "Epoch 105, CIFAR-10 Batch 3:  Valid loss: 0.8653 Valid acc: 0.7024 Train loss: 0.4592 Train acc: 0.9250\n",
      "Epoch 105, CIFAR-10 Batch 4:  Valid loss: 0.8813 Valid acc: 0.6962 Train loss: 0.5189 Train acc: 0.8750\n",
      "Epoch 105, CIFAR-10 Batch 5:  Valid loss: 0.8999 Valid acc: 0.6832 Train loss: 0.6048 Train acc: 0.8750\n",
      "Epoch 106, CIFAR-10 Batch 1:  Valid loss: 0.8549 Valid acc: 0.7056 Train loss: 0.6090 Train acc: 0.7750\n",
      "Epoch 106, CIFAR-10 Batch 2:  Valid loss: 0.8734 Valid acc: 0.6994 Train loss: 0.6093 Train acc: 0.8000\n",
      "Epoch 106, CIFAR-10 Batch 3:  Valid loss: 0.9141 Valid acc: 0.6916 Train loss: 0.5314 Train acc: 0.9000\n",
      "Epoch 106, CIFAR-10 Batch 4:  Valid loss: 0.8683 Valid acc: 0.7062 Train loss: 0.4731 Train acc: 0.8750\n",
      "Epoch 106, CIFAR-10 Batch 5:  Valid loss: 0.8788 Valid acc: 0.7054 Train loss: 0.6058 Train acc: 0.8500\n",
      "Epoch 107, CIFAR-10 Batch 1:  Valid loss: 0.8651 Valid acc: 0.7094 Train loss: 0.6120 Train acc: 0.8000\n",
      "Epoch 107, CIFAR-10 Batch 2:  Valid loss: 0.9124 Valid acc: 0.6876 Train loss: 0.6023 Train acc: 0.7750\n",
      "Epoch 107, CIFAR-10 Batch 3:  Valid loss: 0.9089 Valid acc: 0.6928 Train loss: 0.5328 Train acc: 0.9000\n",
      "Epoch 107, CIFAR-10 Batch 4:  Valid loss: 0.8740 Valid acc: 0.7000 Train loss: 0.4708 Train acc: 0.9000\n",
      "Epoch 107, CIFAR-10 Batch 5:  Valid loss: 0.8968 Valid acc: 0.6932 Train loss: 0.6303 Train acc: 0.8750\n",
      "Epoch 108, CIFAR-10 Batch 1:  Valid loss: 0.8606 Valid acc: 0.7106 Train loss: 0.5889 Train acc: 0.7750\n",
      "Epoch 108, CIFAR-10 Batch 2:  Valid loss: 0.8608 Valid acc: 0.7022 Train loss: 0.6269 Train acc: 0.7750\n",
      "Epoch 108, CIFAR-10 Batch 3:  Valid loss: 0.8833 Valid acc: 0.7012 Train loss: 0.4973 Train acc: 0.9500\n",
      "Epoch 108, CIFAR-10 Batch 4:  Valid loss: 0.8552 Valid acc: 0.7132 Train loss: 0.4819 Train acc: 0.9000\n",
      "Epoch 108, CIFAR-10 Batch 5:  Valid loss: 0.8628 Valid acc: 0.7094 Train loss: 0.5778 Train acc: 0.8500\n",
      "Epoch 109, CIFAR-10 Batch 1:  Valid loss: 0.8543 Valid acc: 0.7120 Train loss: 0.6278 Train acc: 0.7750\n",
      "Epoch 109, CIFAR-10 Batch 2:  Valid loss: 0.8855 Valid acc: 0.6996 Train loss: 0.5984 Train acc: 0.8000\n",
      "Epoch 109, CIFAR-10 Batch 3:  Valid loss: 0.8782 Valid acc: 0.6966 Train loss: 0.4914 Train acc: 0.9500\n",
      "Epoch 109, CIFAR-10 Batch 4:  Valid loss: 0.8560 Valid acc: 0.7166 Train loss: 0.4892 Train acc: 0.9000\n",
      "Epoch 109, CIFAR-10 Batch 5:  Valid loss: 0.8686 Valid acc: 0.7030 Train loss: 0.6109 Train acc: 0.8500\n",
      "Epoch 110, CIFAR-10 Batch 1:  Valid loss: 0.8740 Valid acc: 0.7008 Train loss: 0.6827 Train acc: 0.7500\n",
      "Epoch 110, CIFAR-10 Batch 2:  Valid loss: 0.9031 Valid acc: 0.6920 Train loss: 0.6104 Train acc: 0.7750\n",
      "Epoch 110, CIFAR-10 Batch 3:  Valid loss: 0.8618 Valid acc: 0.7058 Train loss: 0.4746 Train acc: 0.9000\n",
      "Epoch 110, CIFAR-10 Batch 4:  Valid loss: 0.8677 Valid acc: 0.7068 Train loss: 0.4501 Train acc: 0.8750\n",
      "Epoch 110, CIFAR-10 Batch 5:  Valid loss: 0.8902 Valid acc: 0.6956 Train loss: 0.6004 Train acc: 0.8500\n",
      "Epoch 111, CIFAR-10 Batch 1:  Valid loss: 0.8634 Valid acc: 0.7028 Train loss: 0.6640 Train acc: 0.7500\n",
      "Epoch 111, CIFAR-10 Batch 2:  Valid loss: 0.9159 Valid acc: 0.6958 Train loss: 0.6074 Train acc: 0.7750\n",
      "Epoch 111, CIFAR-10 Batch 3:  Valid loss: 0.8694 Valid acc: 0.7054 Train loss: 0.5018 Train acc: 0.9000\n",
      "Epoch 111, CIFAR-10 Batch 4:  Valid loss: 0.8630 Valid acc: 0.7138 Train loss: 0.5052 Train acc: 0.8250\n",
      "Epoch 111, CIFAR-10 Batch 5:  Valid loss: 0.8944 Valid acc: 0.6926 Train loss: 0.5807 Train acc: 0.8500\n",
      "Epoch 112, CIFAR-10 Batch 1:  Valid loss: 0.8726 Valid acc: 0.7068 Train loss: 0.6676 Train acc: 0.7500\n",
      "Epoch 112, CIFAR-10 Batch 2:  Valid loss: 0.8817 Valid acc: 0.7086 Train loss: 0.6063 Train acc: 0.7750\n",
      "Epoch 112, CIFAR-10 Batch 3:  Valid loss: 0.8606 Valid acc: 0.7046 Train loss: 0.4771 Train acc: 0.9000\n",
      "Epoch 112, CIFAR-10 Batch 4:  Valid loss: 0.8752 Valid acc: 0.7030 Train loss: 0.5065 Train acc: 0.9000\n",
      "Epoch 112, CIFAR-10 Batch 5:  Valid loss: 0.9381 Valid acc: 0.6838 Train loss: 0.5937 Train acc: 0.8750\n",
      "Epoch 113, CIFAR-10 Batch 1:  Valid loss: 0.8606 Valid acc: 0.7132 Train loss: 0.6340 Train acc: 0.7750\n",
      "Epoch 113, CIFAR-10 Batch 2:  Valid loss: 0.8789 Valid acc: 0.7058 Train loss: 0.5483 Train acc: 0.8250\n",
      "Epoch 113, CIFAR-10 Batch 3:  Valid loss: 0.8687 Valid acc: 0.7064 Train loss: 0.4821 Train acc: 0.8750\n",
      "Epoch 113, CIFAR-10 Batch 4:  Valid loss: 0.8514 Valid acc: 0.7122 Train loss: 0.5137 Train acc: 0.8000\n",
      "Epoch 113, CIFAR-10 Batch 5:  Valid loss: 0.8830 Valid acc: 0.6974 Train loss: 0.5541 Train acc: 0.8750\n",
      "Epoch 114, CIFAR-10 Batch 1:  Valid loss: 0.8659 Valid acc: 0.7094 Train loss: 0.6231 Train acc: 0.7500\n",
      "Epoch 114, CIFAR-10 Batch 2:  Valid loss: 0.8773 Valid acc: 0.7002 Train loss: 0.5897 Train acc: 0.8000\n",
      "Epoch 114, CIFAR-10 Batch 3:  Valid loss: 0.8758 Valid acc: 0.7010 Train loss: 0.4998 Train acc: 0.9250\n",
      "Epoch 114, CIFAR-10 Batch 4:  Valid loss: 0.8650 Valid acc: 0.7102 Train loss: 0.4763 Train acc: 0.8500\n",
      "Epoch 114, CIFAR-10 Batch 5:  Valid loss: 0.8871 Valid acc: 0.6944 Train loss: 0.5585 Train acc: 0.8750\n",
      "Epoch 115, CIFAR-10 Batch 1:  Valid loss: 0.8590 Valid acc: 0.7100 Train loss: 0.6104 Train acc: 0.7500\n",
      "Epoch 115, CIFAR-10 Batch 2:  Valid loss: 0.9034 Valid acc: 0.6890 Train loss: 0.5870 Train acc: 0.7500\n",
      "Epoch 115, CIFAR-10 Batch 3:  Valid loss: 0.8978 Valid acc: 0.6906 Train loss: 0.5288 Train acc: 0.8750\n",
      "Epoch 115, CIFAR-10 Batch 4:  Valid loss: 0.8984 Valid acc: 0.6928 Train loss: 0.5275 Train acc: 0.8500\n",
      "Epoch 115, CIFAR-10 Batch 5:  Valid loss: 0.8693 Valid acc: 0.7000 Train loss: 0.5290 Train acc: 0.8500\n",
      "Epoch 116, CIFAR-10 Batch 1:  Valid loss: 0.8694 Valid acc: 0.7070 Train loss: 0.6078 Train acc: 0.7500\n",
      "Epoch 116, CIFAR-10 Batch 2:  Valid loss: 0.8911 Valid acc: 0.6936 Train loss: 0.6183 Train acc: 0.8000\n",
      "Epoch 116, CIFAR-10 Batch 3:  Valid loss: 0.8581 Valid acc: 0.7076 Train loss: 0.4591 Train acc: 0.9000\n",
      "Epoch 116, CIFAR-10 Batch 4:  Valid loss: 0.8789 Valid acc: 0.7012 Train loss: 0.4950 Train acc: 0.8250\n",
      "Epoch 116, CIFAR-10 Batch 5:  Valid loss: 0.8961 Valid acc: 0.6844 Train loss: 0.5495 Train acc: 0.8750\n",
      "Epoch 117, CIFAR-10 Batch 1:  Valid loss: 0.8580 Valid acc: 0.7124 Train loss: 0.6043 Train acc: 0.8000\n",
      "Epoch 117, CIFAR-10 Batch 2:  Valid loss: 0.8534 Valid acc: 0.7118 Train loss: 0.5798 Train acc: 0.8250\n",
      "Epoch 117, CIFAR-10 Batch 3:  Valid loss: 0.8611 Valid acc: 0.7036 Train loss: 0.4298 Train acc: 0.9500\n",
      "Epoch 117, CIFAR-10 Batch 4:  Valid loss: 0.8751 Valid acc: 0.6994 Train loss: 0.4882 Train acc: 0.8750\n",
      "Epoch 117, CIFAR-10 Batch 5:  Valid loss: 0.8683 Valid acc: 0.7012 Train loss: 0.5435 Train acc: 0.8750\n",
      "Epoch 118, CIFAR-10 Batch 1:  Valid loss: 0.8440 Valid acc: 0.7146 Train loss: 0.6833 Train acc: 0.7750\n",
      "Epoch 118, CIFAR-10 Batch 2:  Valid loss: 0.8791 Valid acc: 0.7066 Train loss: 0.5898 Train acc: 0.8000\n",
      "Epoch 118, CIFAR-10 Batch 3:  Valid loss: 0.8742 Valid acc: 0.7026 Train loss: 0.4854 Train acc: 0.9000\n",
      "Epoch 118, CIFAR-10 Batch 4:  Valid loss: 0.8737 Valid acc: 0.7018 Train loss: 0.4719 Train acc: 0.8750\n",
      "Epoch 118, CIFAR-10 Batch 5:  Valid loss: 0.8905 Valid acc: 0.6898 Train loss: 0.5432 Train acc: 0.8500\n",
      "Epoch 119, CIFAR-10 Batch 1:  Valid loss: 0.8525 Valid acc: 0.7086 Train loss: 0.6359 Train acc: 0.7750\n",
      "Epoch 119, CIFAR-10 Batch 2:  Valid loss: 0.8606 Valid acc: 0.7110 Train loss: 0.5743 Train acc: 0.7750\n",
      "Epoch 119, CIFAR-10 Batch 3:  Valid loss: 0.8566 Valid acc: 0.7098 Train loss: 0.4550 Train acc: 0.9250\n",
      "Epoch 119, CIFAR-10 Batch 4:  Valid loss: 0.8404 Valid acc: 0.7168 Train loss: 0.4640 Train acc: 0.8500\n",
      "Epoch 119, CIFAR-10 Batch 5:  Valid loss: 0.8863 Valid acc: 0.6946 Train loss: 0.5490 Train acc: 0.9000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Valid loss: 0.8369 Valid acc: 0.7132 Train loss: 0.6169 Train acc: 0.7750\n",
      "Epoch 120, CIFAR-10 Batch 2:  Valid loss: 0.8558 Valid acc: 0.7126 Train loss: 0.5845 Train acc: 0.7750\n",
      "Epoch 120, CIFAR-10 Batch 3:  Valid loss: 0.8527 Valid acc: 0.7086 Train loss: 0.4300 Train acc: 0.9250\n",
      "Epoch 120, CIFAR-10 Batch 4:  Valid loss: 0.8653 Valid acc: 0.7076 Train loss: 0.5245 Train acc: 0.8750\n",
      "Epoch 120, CIFAR-10 Batch 5:  Valid loss: 0.9024 Valid acc: 0.6892 Train loss: 0.5778 Train acc: 0.8500\n",
      "Epoch 121, CIFAR-10 Batch 1:  Valid loss: 0.8361 Valid acc: 0.7156 Train loss: 0.6094 Train acc: 0.7500\n",
      "Epoch 121, CIFAR-10 Batch 2:  Valid loss: 0.8605 Valid acc: 0.7170 Train loss: 0.5858 Train acc: 0.8250\n",
      "Epoch 121, CIFAR-10 Batch 3:  Valid loss: 0.8545 Valid acc: 0.7092 Train loss: 0.4640 Train acc: 0.9500\n",
      "Epoch 121, CIFAR-10 Batch 4:  Valid loss: 0.8526 Valid acc: 0.7198 Train loss: 0.4847 Train acc: 0.9250\n",
      "Epoch 121, CIFAR-10 Batch 5:  Valid loss: 0.8890 Valid acc: 0.6972 Train loss: 0.5077 Train acc: 0.8500\n",
      "Epoch 122, CIFAR-10 Batch 1:  Valid loss: 0.8547 Valid acc: 0.7094 Train loss: 0.6255 Train acc: 0.8000\n",
      "Epoch 122, CIFAR-10 Batch 2:  Valid loss: 0.8593 Valid acc: 0.7106 Train loss: 0.5652 Train acc: 0.8500\n",
      "Epoch 122, CIFAR-10 Batch 3:  Valid loss: 0.8480 Valid acc: 0.7070 Train loss: 0.4393 Train acc: 0.9500\n",
      "Epoch 122, CIFAR-10 Batch 4:  Valid loss: 0.8657 Valid acc: 0.7006 Train loss: 0.4761 Train acc: 0.8750\n",
      "Epoch 122, CIFAR-10 Batch 5:  Valid loss: 0.8675 Valid acc: 0.6944 Train loss: 0.5365 Train acc: 0.9000\n",
      "Epoch 123, CIFAR-10 Batch 1:  Valid loss: 0.8441 Valid acc: 0.7118 Train loss: 0.6183 Train acc: 0.7750\n",
      "Epoch 123, CIFAR-10 Batch 2:  Valid loss: 0.8747 Valid acc: 0.7058 Train loss: 0.5312 Train acc: 0.8500\n",
      "Epoch 123, CIFAR-10 Batch 3:  Valid loss: 0.8753 Valid acc: 0.7094 Train loss: 0.4841 Train acc: 0.9500\n",
      "Epoch 123, CIFAR-10 Batch 4:  Valid loss: 0.8587 Valid acc: 0.7166 Train loss: 0.4604 Train acc: 0.9000\n",
      "Epoch 123, CIFAR-10 Batch 5:  Valid loss: 0.8664 Valid acc: 0.7048 Train loss: 0.5170 Train acc: 0.8750\n",
      "Epoch 124, CIFAR-10 Batch 1:  Valid loss: 0.8445 Valid acc: 0.7176 Train loss: 0.6510 Train acc: 0.7750\n",
      "Epoch 124, CIFAR-10 Batch 2:  Valid loss: 0.8740 Valid acc: 0.7060 Train loss: 0.5313 Train acc: 0.8000\n",
      "Epoch 124, CIFAR-10 Batch 3:  Valid loss: 0.8501 Valid acc: 0.7078 Train loss: 0.4575 Train acc: 0.9000\n",
      "Epoch 124, CIFAR-10 Batch 4:  Valid loss: 0.8435 Valid acc: 0.7226 Train loss: 0.4751 Train acc: 0.8750\n",
      "Epoch 124, CIFAR-10 Batch 5:  Valid loss: 0.8515 Valid acc: 0.7096 Train loss: 0.4987 Train acc: 0.8750\n",
      "Epoch 125, CIFAR-10 Batch 1:  Valid loss: 0.8505 Valid acc: 0.7104 Train loss: 0.6640 Train acc: 0.7250\n",
      "Epoch 125, CIFAR-10 Batch 2:  Valid loss: 0.8584 Valid acc: 0.7102 Train loss: 0.5233 Train acc: 0.8250\n",
      "Epoch 125, CIFAR-10 Batch 3:  Valid loss: 0.8549 Valid acc: 0.7104 Train loss: 0.4738 Train acc: 0.9000\n",
      "Epoch 125, CIFAR-10 Batch 4:  Valid loss: 0.8448 Valid acc: 0.7180 Train loss: 0.4466 Train acc: 0.9000\n",
      "Epoch 125, CIFAR-10 Batch 5:  Valid loss: 0.8603 Valid acc: 0.7042 Train loss: 0.5075 Train acc: 0.9000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Valid loss: 0.8295 Valid acc: 0.7206 Train loss: 0.6011 Train acc: 0.7750\n",
      "Epoch 126, CIFAR-10 Batch 2:  Valid loss: 0.8631 Valid acc: 0.7086 Train loss: 0.5375 Train acc: 0.8000\n",
      "Epoch 126, CIFAR-10 Batch 3:  Valid loss: 0.8568 Valid acc: 0.7046 Train loss: 0.4564 Train acc: 0.9500\n",
      "Epoch 126, CIFAR-10 Batch 4:  Valid loss: 0.8559 Valid acc: 0.7178 Train loss: 0.4978 Train acc: 0.8750\n",
      "Epoch 126, CIFAR-10 Batch 5:  Valid loss: 0.8725 Valid acc: 0.7032 Train loss: 0.5444 Train acc: 0.8750\n",
      "Epoch 127, CIFAR-10 Batch 1:  Valid loss: 0.8451 Valid acc: 0.7082 Train loss: 0.6374 Train acc: 0.8000\n",
      "Epoch 127, CIFAR-10 Batch 2:  Valid loss: 0.8887 Valid acc: 0.7018 Train loss: 0.5042 Train acc: 0.8250\n",
      "Epoch 127, CIFAR-10 Batch 3:  Valid loss: 0.8688 Valid acc: 0.7106 Train loss: 0.4569 Train acc: 0.9500\n",
      "Epoch 127, CIFAR-10 Batch 4:  Valid loss: 0.8485 Valid acc: 0.7146 Train loss: 0.4820 Train acc: 0.9000\n",
      "Epoch 127, CIFAR-10 Batch 5:  Valid loss: 0.8640 Valid acc: 0.7066 Train loss: 0.5226 Train acc: 0.8750\n",
      "Epoch 128, CIFAR-10 Batch 1:  Valid loss: 0.8497 Valid acc: 0.7188 Train loss: 0.6011 Train acc: 0.8000\n",
      "Epoch 128, CIFAR-10 Batch 2:  Valid loss: 0.8515 Valid acc: 0.7080 Train loss: 0.5744 Train acc: 0.8500\n",
      "Epoch 128, CIFAR-10 Batch 3:  Valid loss: 0.8757 Valid acc: 0.6996 Train loss: 0.4786 Train acc: 0.9000\n",
      "Epoch 128, CIFAR-10 Batch 4:  Valid loss: 0.8434 Valid acc: 0.7158 Train loss: 0.4693 Train acc: 0.8750\n",
      "Epoch 128, CIFAR-10 Batch 5:  Valid loss: 0.8549 Valid acc: 0.7076 Train loss: 0.5176 Train acc: 0.8750\n",
      "Epoch 129, CIFAR-10 Batch 1:  Valid loss: 0.8345 Valid acc: 0.7224 Train loss: 0.6085 Train acc: 0.7750\n",
      "Epoch 129, CIFAR-10 Batch 2:  Valid loss: 0.8532 Valid acc: 0.7118 Train loss: 0.5227 Train acc: 0.8000\n",
      "Epoch 129, CIFAR-10 Batch 3:  Valid loss: 0.8478 Valid acc: 0.7158 Train loss: 0.4618 Train acc: 0.9250\n",
      "Epoch 129, CIFAR-10 Batch 4:  Valid loss: 0.8527 Valid acc: 0.7164 Train loss: 0.4657 Train acc: 0.8500\n",
      "Epoch 129, CIFAR-10 Batch 5:  Valid loss: 0.8769 Valid acc: 0.6964 Train loss: 0.5416 Train acc: 0.8500\n",
      "Epoch 130, CIFAR-10 Batch 1:  Valid loss: 0.8311 Valid acc: 0.7262 Train loss: 0.6368 Train acc: 0.7750\n",
      "Epoch 130, CIFAR-10 Batch 2:  Valid loss: 0.8506 Valid acc: 0.7222 Train loss: 0.5215 Train acc: 0.8000\n",
      "Epoch 130, CIFAR-10 Batch 3:  Valid loss: 0.8784 Valid acc: 0.7046 Train loss: 0.4966 Train acc: 0.8750\n",
      "Epoch 130, CIFAR-10 Batch 4:  Valid loss: 0.8539 Valid acc: 0.7160 Train loss: 0.5205 Train acc: 0.8500\n",
      "Epoch 130, CIFAR-10 Batch 5:  Valid loss: 0.8515 Valid acc: 0.7130 Train loss: 0.5123 Train acc: 0.9000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Valid loss: 0.8363 Valid acc: 0.7180 Train loss: 0.6033 Train acc: 0.7750\n",
      "Epoch 131, CIFAR-10 Batch 2:  Valid loss: 0.8695 Valid acc: 0.7074 Train loss: 0.5534 Train acc: 0.8000\n",
      "Epoch 131, CIFAR-10 Batch 3:  Valid loss: 0.8331 Valid acc: 0.7204 Train loss: 0.4530 Train acc: 0.9250\n",
      "Epoch 131, CIFAR-10 Batch 4:  Valid loss: 0.8521 Valid acc: 0.7096 Train loss: 0.4295 Train acc: 0.9000\n",
      "Epoch 131, CIFAR-10 Batch 5:  Valid loss: 0.8478 Valid acc: 0.7152 Train loss: 0.5033 Train acc: 0.8750\n",
      "Epoch 132, CIFAR-10 Batch 1:  Valid loss: 0.8381 Valid acc: 0.7192 Train loss: 0.5588 Train acc: 0.8000\n",
      "Epoch 132, CIFAR-10 Batch 2:  Valid loss: 0.8966 Valid acc: 0.7070 Train loss: 0.5583 Train acc: 0.8500\n",
      "Epoch 132, CIFAR-10 Batch 3:  Valid loss: 0.8433 Valid acc: 0.7130 Train loss: 0.4836 Train acc: 0.9000\n",
      "Epoch 132, CIFAR-10 Batch 4:  Valid loss: 0.8419 Valid acc: 0.7232 Train loss: 0.4219 Train acc: 0.9250\n",
      "Epoch 132, CIFAR-10 Batch 5:  Valid loss: 0.8661 Valid acc: 0.7028 Train loss: 0.4979 Train acc: 0.8750\n",
      "Epoch 133, CIFAR-10 Batch 1:  Valid loss: 0.8443 Valid acc: 0.7142 Train loss: 0.6599 Train acc: 0.7500\n",
      "Epoch 133, CIFAR-10 Batch 2:  Valid loss: 0.9009 Valid acc: 0.6900 Train loss: 0.5832 Train acc: 0.7750\n",
      "Epoch 133, CIFAR-10 Batch 3:  Valid loss: 0.8563 Valid acc: 0.7148 Train loss: 0.4871 Train acc: 0.9250\n",
      "Epoch 133, CIFAR-10 Batch 4:  Valid loss: 0.8479 Valid acc: 0.7174 Train loss: 0.4343 Train acc: 0.9000\n",
      "Epoch 133, CIFAR-10 Batch 5:  Valid loss: 0.8541 Valid acc: 0.7106 Train loss: 0.4907 Train acc: 0.9000\n",
      "Epoch 134, CIFAR-10 Batch 1:  Valid loss: 0.8498 Valid acc: 0.7086 Train loss: 0.6496 Train acc: 0.7250\n",
      "Epoch 134, CIFAR-10 Batch 2:  Valid loss: 0.8551 Valid acc: 0.7126 Train loss: 0.5267 Train acc: 0.8750\n",
      "Epoch 134, CIFAR-10 Batch 3:  Valid loss: 0.8718 Valid acc: 0.7002 Train loss: 0.4674 Train acc: 0.9000\n",
      "Epoch 134, CIFAR-10 Batch 4:  Valid loss: 0.8318 Valid acc: 0.7176 Train loss: 0.4403 Train acc: 0.9250\n",
      "Epoch 134, CIFAR-10 Batch 5:  Valid loss: 0.8613 Valid acc: 0.7098 Train loss: 0.4890 Train acc: 0.9000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Valid loss: 0.8373 Valid acc: 0.7172 Train loss: 0.6397 Train acc: 0.7500\n",
      "Epoch 135, CIFAR-10 Batch 2:  Valid loss: 0.8620 Valid acc: 0.7036 Train loss: 0.5973 Train acc: 0.8000\n",
      "Epoch 135, CIFAR-10 Batch 3:  Valid loss: 0.8558 Valid acc: 0.7114 Train loss: 0.4712 Train acc: 0.9000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Valid loss: 0.8259 Valid acc: 0.7220 Train loss: 0.4276 Train acc: 0.9000\n",
      "Epoch 135, CIFAR-10 Batch 5:  Valid loss: 0.8584 Valid acc: 0.7034 Train loss: 0.5194 Train acc: 0.8750\n",
      "Epoch 136, CIFAR-10 Batch 1:  Valid loss: 0.8446 Valid acc: 0.7122 Train loss: 0.6479 Train acc: 0.7750\n",
      "Epoch 136, CIFAR-10 Batch 2:  Valid loss: 0.8961 Valid acc: 0.6936 Train loss: 0.5454 Train acc: 0.8000\n",
      "Epoch 136, CIFAR-10 Batch 3:  Valid loss: 0.8291 Valid acc: 0.7250 Train loss: 0.4337 Train acc: 0.9000\n",
      "Epoch 136, CIFAR-10 Batch 4:  Valid loss: 0.8348 Valid acc: 0.7200 Train loss: 0.4417 Train acc: 0.9250\n",
      "Epoch 136, CIFAR-10 Batch 5:  Valid loss: 0.8351 Valid acc: 0.7180 Train loss: 0.4631 Train acc: 0.9250\n",
      "Epoch 137, CIFAR-10 Batch 1:  Valid loss: 0.8313 Valid acc: 0.7196 Train loss: 0.6211 Train acc: 0.7750\n",
      "Epoch 137, CIFAR-10 Batch 2:  Valid loss: 0.8408 Valid acc: 0.7138 Train loss: 0.5297 Train acc: 0.8000\n",
      "Epoch 137, CIFAR-10 Batch 3:  Valid loss: 0.8534 Valid acc: 0.7064 Train loss: 0.4481 Train acc: 0.9500\n",
      "Epoch 137, CIFAR-10 Batch 4:  Valid loss: 0.8306 Valid acc: 0.7166 Train loss: 0.4918 Train acc: 0.8500\n",
      "Epoch 137, CIFAR-10 Batch 5:  Valid loss: 0.8590 Valid acc: 0.7024 Train loss: 0.4922 Train acc: 0.9000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Valid loss: 0.8104 Valid acc: 0.7306 Train loss: 0.6255 Train acc: 0.7750\n",
      "Epoch 138, CIFAR-10 Batch 2:  Valid loss: 0.8333 Valid acc: 0.7236 Train loss: 0.5392 Train acc: 0.8000\n",
      "Epoch 138, CIFAR-10 Batch 3:  Valid loss: 0.8481 Valid acc: 0.7138 Train loss: 0.4595 Train acc: 0.9000\n",
      "Epoch 138, CIFAR-10 Batch 4:  Valid loss: 0.8301 Valid acc: 0.7256 Train loss: 0.4538 Train acc: 0.9000\n",
      "Epoch 138, CIFAR-10 Batch 5:  Valid loss: 0.8462 Valid acc: 0.7106 Train loss: 0.4894 Train acc: 0.9000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Valid loss: 0.8276 Valid acc: 0.7272 Train loss: 0.6026 Train acc: 0.8000\n",
      "Epoch 139, CIFAR-10 Batch 2:  Valid loss: 0.8470 Valid acc: 0.7204 Train loss: 0.5010 Train acc: 0.8000\n",
      "Epoch 139, CIFAR-10 Batch 3:  Valid loss: 0.8762 Valid acc: 0.7052 Train loss: 0.4813 Train acc: 0.9000\n",
      "Epoch 139, CIFAR-10 Batch 4:  Valid loss: 0.8623 Valid acc: 0.7086 Train loss: 0.4658 Train acc: 0.9000\n",
      "Epoch 139, CIFAR-10 Batch 5:  Valid loss: 0.8633 Valid acc: 0.7074 Train loss: 0.4470 Train acc: 0.9250\n",
      "Epoch 140, CIFAR-10 Batch 1:  Valid loss: 0.8137 Valid acc: 0.7236 Train loss: 0.5965 Train acc: 0.8000\n",
      "Epoch 140, CIFAR-10 Batch 2:  Valid loss: 0.8438 Valid acc: 0.7168 Train loss: 0.5179 Train acc: 0.8500\n",
      "Epoch 140, CIFAR-10 Batch 3:  Valid loss: 0.8470 Valid acc: 0.7088 Train loss: 0.4668 Train acc: 0.8750\n",
      "Epoch 140, CIFAR-10 Batch 4:  Valid loss: 0.8406 Valid acc: 0.7144 Train loss: 0.4530 Train acc: 0.8750\n",
      "Epoch 140, CIFAR-10 Batch 5:  Valid loss: 0.8677 Valid acc: 0.7048 Train loss: 0.4748 Train acc: 0.9250\n",
      "Epoch 141, CIFAR-10 Batch 1:  Valid loss: 0.8329 Valid acc: 0.7186 Train loss: 0.6033 Train acc: 0.8000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Valid loss: 0.8757 Valid acc: 0.6930 Train loss: 0.6113 Train acc: 0.7750\n",
      "Epoch 141, CIFAR-10 Batch 3:  Valid loss: 0.8245 Valid acc: 0.7270 Train loss: 0.4671 Train acc: 0.8750\n",
      "Epoch 141, CIFAR-10 Batch 4:  Valid loss: 0.8498 Valid acc: 0.7090 Train loss: 0.4796 Train acc: 0.8750\n",
      "Epoch 141, CIFAR-10 Batch 5:  Valid loss: 0.8314 Valid acc: 0.7220 Train loss: 0.4439 Train acc: 0.9250\n",
      "Epoch 142, CIFAR-10 Batch 1:  Valid loss: 0.8363 Valid acc: 0.7098 Train loss: 0.5917 Train acc: 0.8000\n",
      "Epoch 142, CIFAR-10 Batch 2:  Valid loss: 0.8330 Valid acc: 0.7256 Train loss: 0.5169 Train acc: 0.8500\n",
      "Epoch 142, CIFAR-10 Batch 3:  Valid loss: 0.8299 Valid acc: 0.7220 Train loss: 0.4223 Train acc: 0.9500\n",
      "Epoch 142, CIFAR-10 Batch 4:  Valid loss: 0.8437 Valid acc: 0.7146 Train loss: 0.4817 Train acc: 0.8500\n",
      "Epoch 142, CIFAR-10 Batch 5:  Valid loss: 0.8867 Valid acc: 0.6988 Train loss: 0.5614 Train acc: 0.8250\n",
      "Epoch 143, CIFAR-10 Batch 1:  Valid loss: 0.8286 Valid acc: 0.7132 Train loss: 0.5685 Train acc: 0.7750\n",
      "Epoch 143, CIFAR-10 Batch 2:  Valid loss: 0.8719 Valid acc: 0.7072 Train loss: 0.5385 Train acc: 0.8250\n",
      "Epoch 143, CIFAR-10 Batch 3:  Valid loss: 0.8474 Valid acc: 0.7154 Train loss: 0.4555 Train acc: 0.9250\n",
      "Epoch 143, CIFAR-10 Batch 4:  Valid loss: 0.8595 Valid acc: 0.7096 Train loss: 0.4471 Train acc: 0.9000\n",
      "Epoch 143, CIFAR-10 Batch 5:  Valid loss: 0.8410 Valid acc: 0.7170 Train loss: 0.5196 Train acc: 0.8250\n",
      "Epoch 144, CIFAR-10 Batch 1:  Valid loss: 0.8520 Valid acc: 0.7046 Train loss: 0.5830 Train acc: 0.8000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Valid loss: 0.8803 Valid acc: 0.7064 Train loss: 0.5471 Train acc: 0.8250\n",
      "Epoch 144, CIFAR-10 Batch 3:  Valid loss: 0.8390 Valid acc: 0.7112 Train loss: 0.4481 Train acc: 0.8750\n",
      "Epoch 144, CIFAR-10 Batch 4:  Valid loss: 0.8425 Valid acc: 0.7200 Train loss: 0.4635 Train acc: 0.9250\n",
      "Epoch 144, CIFAR-10 Batch 5:  Valid loss: 0.8165 Valid acc: 0.7234 Train loss: 0.4296 Train acc: 0.9250\n",
      "Epoch 145, CIFAR-10 Batch 1:  Valid loss: 0.8482 Valid acc: 0.7084 Train loss: 0.6311 Train acc: 0.7250\n",
      "Epoch 145, CIFAR-10 Batch 2:  Valid loss: 0.8293 Valid acc: 0.7254 Train loss: 0.5252 Train acc: 0.8500\n",
      "Epoch 145, CIFAR-10 Batch 3:  Valid loss: 0.8476 Valid acc: 0.7186 Train loss: 0.4824 Train acc: 0.8250\n",
      "Epoch 145, CIFAR-10 Batch 4:  Valid loss: 0.8280 Valid acc: 0.7192 Train loss: 0.4390 Train acc: 0.9250\n",
      "Epoch 145, CIFAR-10 Batch 5:  Valid loss: 0.8523 Valid acc: 0.7100 Train loss: 0.4375 Train acc: 0.9250\n",
      "Epoch 146, CIFAR-10 Batch 1:  Valid loss: 0.8316 Valid acc: 0.7146 Train loss: 0.6444 Train acc: 0.8000\n",
      "Epoch 146, CIFAR-10 Batch 2:  Valid loss: 0.8486 Valid acc: 0.7124 Train loss: 0.5434 Train acc: 0.8000\n",
      "Epoch 146, CIFAR-10 Batch 3:  Valid loss: 0.8351 Valid acc: 0.7154 Train loss: 0.4420 Train acc: 0.9000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Valid loss: 0.8341 Valid acc: 0.7102 Train loss: 0.4417 Train acc: 0.9000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Valid loss: 0.8226 Valid acc: 0.7196 Train loss: 0.4520 Train acc: 0.9500\n",
      "Epoch 147, CIFAR-10 Batch 1:  Valid loss: 0.8164 Valid acc: 0.7234 Train loss: 0.6073 Train acc: 0.7250\n",
      "Epoch 147, CIFAR-10 Batch 2:  Valid loss: 0.8347 Valid acc: 0.7242 Train loss: 0.5483 Train acc: 0.8250\n",
      "Epoch 147, CIFAR-10 Batch 3:  Valid loss: 0.8243 Valid acc: 0.7188 Train loss: 0.4386 Train acc: 0.9250\n",
      "Epoch 147, CIFAR-10 Batch 4:  Valid loss: 0.8414 Valid acc: 0.7176 Train loss: 0.4408 Train acc: 0.8500\n",
      "Epoch 147, CIFAR-10 Batch 5:  Valid loss: 0.8577 Valid acc: 0.7070 Train loss: 0.4611 Train acc: 0.9250\n",
      "Epoch 148, CIFAR-10 Batch 1:  Valid loss: 0.8277 Valid acc: 0.7194 Train loss: 0.6068 Train acc: 0.7750\n",
      "Epoch 148, CIFAR-10 Batch 2:  Valid loss: 0.8361 Valid acc: 0.7210 Train loss: 0.5415 Train acc: 0.8000\n",
      "Epoch 148, CIFAR-10 Batch 3:  Valid loss: 0.8066 Valid acc: 0.7236 Train loss: 0.4298 Train acc: 0.9000\n",
      "Epoch 148, CIFAR-10 Batch 4:  Valid loss: 0.8838 Valid acc: 0.7000 Train loss: 0.4978 Train acc: 0.8750\n",
      "Epoch 148, CIFAR-10 Batch 5:  Valid loss: 0.8325 Valid acc: 0.7238 Train loss: 0.4660 Train acc: 0.9000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Valid loss: 0.8272 Valid acc: 0.7240 Train loss: 0.6489 Train acc: 0.7750\n",
      "Epoch 149, CIFAR-10 Batch 2:  Valid loss: 0.8308 Valid acc: 0.7126 Train loss: 0.5280 Train acc: 0.8750\n",
      "Epoch 149, CIFAR-10 Batch 3:  Valid loss: 0.8232 Valid acc: 0.7148 Train loss: 0.4091 Train acc: 0.9750\n",
      "Epoch 149, CIFAR-10 Batch 4:  Valid loss: 0.8408 Valid acc: 0.7198 Train loss: 0.4741 Train acc: 0.8750\n",
      "Epoch 149, CIFAR-10 Batch 5:  Valid loss: 0.8279 Valid acc: 0.7246 Train loss: 0.4306 Train acc: 0.9250\n",
      "Epoch 150, CIFAR-10 Batch 1:  Valid loss: 0.8189 Valid acc: 0.7270 Train loss: 0.6112 Train acc: 0.7750\n",
      "Epoch 150, CIFAR-10 Batch 2:  Valid loss: 0.8478 Valid acc: 0.7172 Train loss: 0.5128 Train acc: 0.8750\n",
      "Epoch 150, CIFAR-10 Batch 3:  Valid loss: 0.8590 Valid acc: 0.7090 Train loss: 0.4885 Train acc: 0.9000\n",
      "Epoch 150, CIFAR-10 Batch 4:  Valid loss: 0.8139 Valid acc: 0.7248 Train loss: 0.4327 Train acc: 0.9250\n",
      "Epoch 150, CIFAR-10 Batch 5:  Valid loss: 0.8494 Valid acc: 0.7096 Train loss: 0.4877 Train acc: 0.9250\n",
      "Epoch 151, CIFAR-10 Batch 1:  Valid loss: 0.8089 Valid acc: 0.7210 Train loss: 0.5422 Train acc: 0.8000\n",
      "Epoch 151, CIFAR-10 Batch 2:  Valid loss: 0.8254 Valid acc: 0.7262 Train loss: 0.4905 Train acc: 0.8750\n",
      "Epoch 151, CIFAR-10 Batch 3:  Valid loss: 0.8817 Valid acc: 0.7002 Train loss: 0.5315 Train acc: 0.9000\n",
      "Epoch 151, CIFAR-10 Batch 4:  Valid loss: 0.8155 Valid acc: 0.7292 Train loss: 0.4241 Train acc: 0.9000\n",
      "Epoch 151, CIFAR-10 Batch 5:  Valid loss: 0.8672 Valid acc: 0.7062 Train loss: 0.4961 Train acc: 0.8750\n",
      "Epoch 152, CIFAR-10 Batch 1:  Valid loss: 0.8153 Valid acc: 0.7286 Train loss: 0.6126 Train acc: 0.7750\n",
      "Epoch 152, CIFAR-10 Batch 2:  Valid loss: 0.8426 Valid acc: 0.7166 Train loss: 0.5545 Train acc: 0.8000\n",
      "Epoch 152, CIFAR-10 Batch 3:  Valid loss: 0.8245 Valid acc: 0.7242 Train loss: 0.4340 Train acc: 0.9750\n",
      "Epoch 152, CIFAR-10 Batch 4:  Valid loss: 0.8288 Valid acc: 0.7186 Train loss: 0.4860 Train acc: 0.9250\n",
      "Epoch 152, CIFAR-10 Batch 5:  Valid loss: 0.8604 Valid acc: 0.7088 Train loss: 0.4530 Train acc: 0.9000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Valid loss: 0.8054 Valid acc: 0.7266 Train loss: 0.6036 Train acc: 0.8000\n",
      "Epoch 153, CIFAR-10 Batch 2:  Valid loss: 0.8367 Valid acc: 0.7190 Train loss: 0.5342 Train acc: 0.8500\n",
      "Epoch 153, CIFAR-10 Batch 3:  Valid loss: 0.8435 Valid acc: 0.7186 Train loss: 0.4243 Train acc: 0.9250\n",
      "Epoch 153, CIFAR-10 Batch 4:  Valid loss: 0.8252 Valid acc: 0.7266 Train loss: 0.4587 Train acc: 0.9000\n",
      "Epoch 153, CIFAR-10 Batch 5:  Valid loss: 0.8164 Valid acc: 0.7288 Train loss: 0.4126 Train acc: 0.9250\n",
      "Epoch 154, CIFAR-10 Batch 1:  Valid loss: 0.8196 Valid acc: 0.7188 Train loss: 0.5719 Train acc: 0.8000\n",
      "Epoch 154, CIFAR-10 Batch 2:  Valid loss: 0.8561 Valid acc: 0.7038 Train loss: 0.5778 Train acc: 0.7750\n",
      "Epoch 154, CIFAR-10 Batch 3:  Valid loss: 0.8430 Valid acc: 0.7110 Train loss: 0.4729 Train acc: 0.8750\n",
      "Epoch 154, CIFAR-10 Batch 4:  Valid loss: 0.8369 Valid acc: 0.7114 Train loss: 0.4653 Train acc: 0.9000\n",
      "Epoch 154, CIFAR-10 Batch 5:  Valid loss: 0.8171 Valid acc: 0.7254 Train loss: 0.4549 Train acc: 0.9000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Valid loss: 0.8323 Valid acc: 0.7176 Train loss: 0.6415 Train acc: 0.7750\n",
      "Epoch 155, CIFAR-10 Batch 2:  Valid loss: 0.8495 Valid acc: 0.7186 Train loss: 0.5117 Train acc: 0.8000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Valid loss: 0.8178 Valid acc: 0.7228 Train loss: 0.4210 Train acc: 0.9500\n",
      "Epoch 155, CIFAR-10 Batch 4:  Valid loss: 0.8185 Valid acc: 0.7288 Train loss: 0.4275 Train acc: 0.9250\n",
      "Epoch 155, CIFAR-10 Batch 5:  Valid loss: 0.8255 Valid acc: 0.7164 Train loss: 0.4376 Train acc: 0.9000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Valid loss: 0.8055 Valid acc: 0.7280 Train loss: 0.5876 Train acc: 0.7750\n",
      "Epoch 156, CIFAR-10 Batch 2:  Valid loss: 0.8428 Valid acc: 0.7158 Train loss: 0.5129 Train acc: 0.8500\n",
      "Epoch 156, CIFAR-10 Batch 3:  Valid loss: 0.8514 Valid acc: 0.7092 Train loss: 0.4322 Train acc: 0.9000\n",
      "Epoch 156, CIFAR-10 Batch 4:  Valid loss: 0.8210 Valid acc: 0.7264 Train loss: 0.4315 Train acc: 0.9000\n",
      "Epoch 156, CIFAR-10 Batch 5:  Valid loss: 0.8217 Valid acc: 0.7190 Train loss: 0.4234 Train acc: 0.9000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Valid loss: 0.8372 Valid acc: 0.7144 Train loss: 0.6384 Train acc: 0.8000\n",
      "Epoch 157, CIFAR-10 Batch 2:  Valid loss: 0.8186 Valid acc: 0.7262 Train loss: 0.5148 Train acc: 0.8000\n",
      "Epoch 157, CIFAR-10 Batch 3:  Valid loss: 0.8253 Valid acc: 0.7132 Train loss: 0.4057 Train acc: 0.9750\n",
      "Epoch 157, CIFAR-10 Batch 4:  Valid loss: 0.8598 Valid acc: 0.7204 Train loss: 0.4476 Train acc: 0.9250\n",
      "Epoch 157, CIFAR-10 Batch 5:  Valid loss: 0.8388 Valid acc: 0.7102 Train loss: 0.4580 Train acc: 0.8500\n",
      "Epoch 158, CIFAR-10 Batch 1:  Valid loss: 0.8617 Valid acc: 0.7038 Train loss: 0.7132 Train acc: 0.7500\n",
      "Epoch 158, CIFAR-10 Batch 2:  Valid loss: 0.8297 Valid acc: 0.7240 Train loss: 0.5126 Train acc: 0.8000\n",
      "Epoch 158, CIFAR-10 Batch 3:  Valid loss: 0.8551 Valid acc: 0.7048 Train loss: 0.4851 Train acc: 0.9250\n",
      "Epoch 158, CIFAR-10 Batch 4:  Valid loss: 0.8148 Valid acc: 0.7294 Train loss: 0.4151 Train acc: 0.9250\n",
      "Epoch 158, CIFAR-10 Batch 5:  Valid loss: 0.8634 Valid acc: 0.7056 Train loss: 0.4636 Train acc: 0.8750\n",
      "Epoch 159, CIFAR-10 Batch 1:  Valid loss: 0.8390 Valid acc: 0.7078 Train loss: 0.5910 Train acc: 0.8000\n",
      "Epoch 159, CIFAR-10 Batch 2:  Valid loss: 0.8338 Valid acc: 0.7204 Train loss: 0.5032 Train acc: 0.8250\n",
      "Epoch 159, CIFAR-10 Batch 3:  Valid loss: 0.8618 Valid acc: 0.7074 Train loss: 0.4593 Train acc: 0.8750\n",
      "Epoch 159, CIFAR-10 Batch 4:  Valid loss: 0.8478 Valid acc: 0.7244 Train loss: 0.4609 Train acc: 0.9000\n",
      "Epoch 159, CIFAR-10 Batch 5:  Valid loss: 0.8549 Valid acc: 0.7090 Train loss: 0.4681 Train acc: 0.9000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Valid loss: 0.8117 Valid acc: 0.7242 Train loss: 0.6119 Train acc: 0.8000\n",
      "Epoch 160, CIFAR-10 Batch 2:  Valid loss: 0.8496 Valid acc: 0.7208 Train loss: 0.5153 Train acc: 0.8500\n",
      "Epoch 160, CIFAR-10 Batch 3:  Valid loss: 0.8259 Valid acc: 0.7160 Train loss: 0.4277 Train acc: 0.9500\n",
      "Epoch 160, CIFAR-10 Batch 4:  Valid loss: 0.8283 Valid acc: 0.7218 Train loss: 0.4277 Train acc: 0.9000\n",
      "Epoch 160, CIFAR-10 Batch 5:  Valid loss: 0.8293 Valid acc: 0.7266 Train loss: 0.4270 Train acc: 0.9250\n",
      "Epoch 161, CIFAR-10 Batch 1:  Valid loss: 0.8068 Valid acc: 0.7240 Train loss: 0.5932 Train acc: 0.7750\n",
      "Epoch 161, CIFAR-10 Batch 2:  Valid loss: 0.8313 Valid acc: 0.7296 Train loss: 0.5301 Train acc: 0.8250\n",
      "Epoch 161, CIFAR-10 Batch 3:  Valid loss: 0.8232 Valid acc: 0.7212 Train loss: 0.4234 Train acc: 0.9500\n",
      "Epoch 161, CIFAR-10 Batch 4:  Valid loss: 0.8347 Valid acc: 0.7194 Train loss: 0.4111 Train acc: 0.9000\n",
      "Epoch 161, CIFAR-10 Batch 5:  Valid loss: 0.8282 Valid acc: 0.7154 Train loss: 0.4373 Train acc: 0.9250\n",
      "Epoch 162, CIFAR-10 Batch 1:  Valid loss: 0.8121 Valid acc: 0.7182 Train loss: 0.5835 Train acc: 0.8000\n",
      "Epoch 162, CIFAR-10 Batch 2:  Valid loss: 0.8014 Valid acc: 0.7264 Train loss: 0.4733 Train acc: 0.8250\n",
      "Epoch 162, CIFAR-10 Batch 3:  Valid loss: 0.8078 Valid acc: 0.7330 Train loss: 0.4152 Train acc: 0.9000\n",
      "Epoch 162, CIFAR-10 Batch 4:  Valid loss: 0.8174 Valid acc: 0.7258 Train loss: 0.4096 Train acc: 0.9250\n",
      "Epoch 162, CIFAR-10 Batch 5:  Valid loss: 0.8468 Valid acc: 0.7142 Train loss: 0.4547 Train acc: 0.9000\n",
      "Epoch 163, CIFAR-10 Batch 1:  Valid loss: 0.8079 Valid acc: 0.7288 Train loss: 0.5515 Train acc: 0.8000\n",
      "Epoch 163, CIFAR-10 Batch 2:  Valid loss: 0.8327 Valid acc: 0.7224 Train loss: 0.4864 Train acc: 0.8250\n",
      "Epoch 163, CIFAR-10 Batch 3:  Valid loss: 0.7967 Valid acc: 0.7304 Train loss: 0.4000 Train acc: 0.8750\n",
      "Epoch 163, CIFAR-10 Batch 4:  Valid loss: 0.8415 Valid acc: 0.7168 Train loss: 0.4456 Train acc: 0.8750\n",
      "Epoch 163, CIFAR-10 Batch 5:  Valid loss: 0.8552 Valid acc: 0.7108 Train loss: 0.4630 Train acc: 0.9250\n",
      "Epoch 164, CIFAR-10 Batch 1:  Valid loss: 0.8150 Valid acc: 0.7244 Train loss: 0.5893 Train acc: 0.7500\n",
      "Epoch 164, CIFAR-10 Batch 2:  Valid loss: 0.8184 Valid acc: 0.7324 Train loss: 0.5362 Train acc: 0.8250\n",
      "Epoch 164, CIFAR-10 Batch 3:  Valid loss: 0.8091 Valid acc: 0.7274 Train loss: 0.4002 Train acc: 0.9250\n",
      "Epoch 164, CIFAR-10 Batch 4:  Valid loss: 0.8239 Valid acc: 0.7246 Train loss: 0.4050 Train acc: 0.9250\n",
      "Epoch 164, CIFAR-10 Batch 5:  Valid loss: 0.8207 Valid acc: 0.7182 Train loss: 0.4355 Train acc: 0.9000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Valid loss: 0.8100 Valid acc: 0.7228 Train loss: 0.5755 Train acc: 0.8000\n",
      "Epoch 165, CIFAR-10 Batch 2:  Valid loss: 0.8123 Valid acc: 0.7284 Train loss: 0.4932 Train acc: 0.8250\n",
      "Epoch 165, CIFAR-10 Batch 3:  Valid loss: 0.8312 Valid acc: 0.7234 Train loss: 0.4588 Train acc: 0.9000\n",
      "Epoch 165, CIFAR-10 Batch 4:  Valid loss: 0.8175 Valid acc: 0.7266 Train loss: 0.4256 Train acc: 0.9000\n",
      "Epoch 165, CIFAR-10 Batch 5:  Valid loss: 0.8210 Valid acc: 0.7196 Train loss: 0.4045 Train acc: 0.9500\n",
      "Epoch 166, CIFAR-10 Batch 1:  Valid loss: 0.7994 Valid acc: 0.7298 Train loss: 0.5513 Train acc: 0.7750\n",
      "Epoch 166, CIFAR-10 Batch 2:  Valid loss: 0.8162 Valid acc: 0.7290 Train loss: 0.4432 Train acc: 0.8500\n",
      "Epoch 166, CIFAR-10 Batch 3:  Valid loss: 0.8194 Valid acc: 0.7204 Train loss: 0.4197 Train acc: 0.9500\n",
      "Epoch 166, CIFAR-10 Batch 4:  Valid loss: 0.8244 Valid acc: 0.7258 Train loss: 0.4281 Train acc: 0.9250\n",
      "Epoch 166, CIFAR-10 Batch 5:  Valid loss: 0.8360 Valid acc: 0.7144 Train loss: 0.4145 Train acc: 0.8750\n",
      "Epoch 167, CIFAR-10 Batch 1:  Valid loss: 0.8198 Valid acc: 0.7196 Train loss: 0.5854 Train acc: 0.8000\n",
      "Epoch 167, CIFAR-10 Batch 2:  Valid loss: 0.8355 Valid acc: 0.7222 Train loss: 0.4687 Train acc: 0.8250\n",
      "Epoch 167, CIFAR-10 Batch 3:  Valid loss: 0.8260 Valid acc: 0.7232 Train loss: 0.4804 Train acc: 0.9000\n",
      "Epoch 167, CIFAR-10 Batch 4:  Valid loss: 0.8198 Valid acc: 0.7246 Train loss: 0.4080 Train acc: 0.9000\n",
      "Epoch 167, CIFAR-10 Batch 5:  Valid loss: 0.8233 Valid acc: 0.7164 Train loss: 0.4024 Train acc: 0.9000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Valid loss: 0.8117 Valid acc: 0.7212 Train loss: 0.5931 Train acc: 0.8000\n",
      "Epoch 168, CIFAR-10 Batch 2:  Valid loss: 0.8522 Valid acc: 0.7094 Train loss: 0.5099 Train acc: 0.8000\n",
      "Epoch 168, CIFAR-10 Batch 3:  Valid loss: 0.8221 Valid acc: 0.7280 Train loss: 0.4974 Train acc: 0.8250\n",
      "Epoch 168, CIFAR-10 Batch 4:  Valid loss: 0.8367 Valid acc: 0.7276 Train loss: 0.4577 Train acc: 0.9000\n",
      "Epoch 168, CIFAR-10 Batch 5:  Valid loss: 0.8483 Valid acc: 0.7098 Train loss: 0.4413 Train acc: 0.9250\n",
      "Epoch 169, CIFAR-10 Batch 1:  Valid loss: 0.7928 Valid acc: 0.7370 Train loss: 0.5585 Train acc: 0.7750\n",
      "Epoch 169, CIFAR-10 Batch 2:  Valid loss: 0.8397 Valid acc: 0.7210 Train loss: 0.5150 Train acc: 0.8500\n",
      "Epoch 169, CIFAR-10 Batch 3:  Valid loss: 0.8224 Valid acc: 0.7144 Train loss: 0.4278 Train acc: 0.9000\n",
      "Epoch 169, CIFAR-10 Batch 4:  Valid loss: 0.8137 Valid acc: 0.7234 Train loss: 0.3935 Train acc: 0.9500\n",
      "Epoch 169, CIFAR-10 Batch 5:  Valid loss: 0.8561 Valid acc: 0.7034 Train loss: 0.4885 Train acc: 0.8500\n",
      "Epoch 170, CIFAR-10 Batch 1:  Valid loss: 0.8056 Valid acc: 0.7296 Train loss: 0.5522 Train acc: 0.8000\n",
      "Epoch 170, CIFAR-10 Batch 2:  Valid loss: 0.8576 Valid acc: 0.7114 Train loss: 0.5323 Train acc: 0.8000\n",
      "Epoch 170, CIFAR-10 Batch 3:  Valid loss: 0.8193 Valid acc: 0.7196 Train loss: 0.4503 Train acc: 0.9000\n",
      "Epoch 170, CIFAR-10 Batch 4:  Valid loss: 0.8228 Valid acc: 0.7240 Train loss: 0.4461 Train acc: 0.8500\n",
      "Epoch 170, CIFAR-10 Batch 5:  Valid loss: 0.8213 Valid acc: 0.7214 Train loss: 0.4133 Train acc: 0.9500\n",
      "Epoch 171, CIFAR-10 Batch 1:  Valid loss: 0.8074 Valid acc: 0.7294 Train loss: 0.5634 Train acc: 0.7750\n",
      "Epoch 171, CIFAR-10 Batch 2:  Valid loss: 0.8280 Valid acc: 0.7208 Train loss: 0.5005 Train acc: 0.8000\n",
      "Epoch 171, CIFAR-10 Batch 3:  Valid loss: 0.8050 Valid acc: 0.7258 Train loss: 0.4175 Train acc: 0.9500\n",
      "Epoch 171, CIFAR-10 Batch 4:  Valid loss: 0.8078 Valid acc: 0.7330 Train loss: 0.3903 Train acc: 0.9500\n",
      "Epoch 171, CIFAR-10 Batch 5:  Valid loss: 0.8262 Valid acc: 0.7298 Train loss: 0.4434 Train acc: 0.9250\n",
      "Epoch 172, CIFAR-10 Batch 1:  Valid loss: 0.8162 Valid acc: 0.7260 Train loss: 0.5775 Train acc: 0.7750\n",
      "Epoch 172, CIFAR-10 Batch 2:  Valid loss: 0.8216 Valid acc: 0.7240 Train loss: 0.4864 Train acc: 0.8500\n",
      "Epoch 172, CIFAR-10 Batch 3:  Valid loss: 0.8227 Valid acc: 0.7212 Train loss: 0.4355 Train acc: 0.9500\n",
      "Epoch 172, CIFAR-10 Batch 4:  Valid loss: 0.8041 Valid acc: 0.7340 Train loss: 0.4252 Train acc: 0.9000\n",
      "Epoch 172, CIFAR-10 Batch 5:  Valid loss: 0.8282 Valid acc: 0.7244 Train loss: 0.4088 Train acc: 0.9000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Valid loss: 0.8125 Valid acc: 0.7236 Train loss: 0.6044 Train acc: 0.7750\n",
      "Epoch 173, CIFAR-10 Batch 2:  Valid loss: 0.8225 Valid acc: 0.7248 Train loss: 0.5307 Train acc: 0.8000\n",
      "Epoch 173, CIFAR-10 Batch 3:  Valid loss: 0.8122 Valid acc: 0.7284 Train loss: 0.4281 Train acc: 0.9250\n",
      "Epoch 173, CIFAR-10 Batch 4:  Valid loss: 0.8111 Valid acc: 0.7246 Train loss: 0.4038 Train acc: 0.9000\n",
      "Epoch 173, CIFAR-10 Batch 5:  Valid loss: 0.8171 Valid acc: 0.7268 Train loss: 0.4202 Train acc: 0.9250\n",
      "Epoch 174, CIFAR-10 Batch 1:  Valid loss: 0.8020 Valid acc: 0.7288 Train loss: 0.5399 Train acc: 0.8000\n",
      "Epoch 174, CIFAR-10 Batch 2:  Valid loss: 0.8041 Valid acc: 0.7340 Train loss: 0.4689 Train acc: 0.8500\n",
      "Epoch 174, CIFAR-10 Batch 3:  Valid loss: 0.8324 Valid acc: 0.7152 Train loss: 0.4483 Train acc: 0.9500\n",
      "Epoch 174, CIFAR-10 Batch 4:  Valid loss: 0.8108 Valid acc: 0.7282 Train loss: 0.4024 Train acc: 0.8750\n",
      "Epoch 174, CIFAR-10 Batch 5:  Valid loss: 0.8104 Valid acc: 0.7270 Train loss: 0.4355 Train acc: 0.8750\n",
      "Epoch 175, CIFAR-10 Batch 1:  Valid loss: 0.8041 Valid acc: 0.7292 Train loss: 0.5854 Train acc: 0.7500\n",
      "Epoch 175, CIFAR-10 Batch 2:  Valid loss: 0.8301 Valid acc: 0.7288 Train loss: 0.4657 Train acc: 0.8250\n",
      "Epoch 175, CIFAR-10 Batch 3:  Valid loss: 0.8242 Valid acc: 0.7218 Train loss: 0.4389 Train acc: 0.9000\n",
      "Epoch 175, CIFAR-10 Batch 4:  Valid loss: 0.8083 Valid acc: 0.7356 Train loss: 0.3837 Train acc: 0.9250\n",
      "Epoch 175, CIFAR-10 Batch 5:  Valid loss: 0.8281 Valid acc: 0.7220 Train loss: 0.3831 Train acc: 0.9750\n",
      "Epoch 176, CIFAR-10 Batch 1:  Valid loss: 0.8139 Valid acc: 0.7272 Train loss: 0.6047 Train acc: 0.7250\n",
      "Epoch 176, CIFAR-10 Batch 2:  Valid loss: 0.8069 Valid acc: 0.7302 Train loss: 0.4707 Train acc: 0.8250\n",
      "Epoch 176, CIFAR-10 Batch 3:  Valid loss: 0.7977 Valid acc: 0.7328 Train loss: 0.3770 Train acc: 0.9500\n",
      "Epoch 176, CIFAR-10 Batch 4:  Valid loss: 0.8080 Valid acc: 0.7356 Train loss: 0.4405 Train acc: 0.9000\n",
      "Epoch 176, CIFAR-10 Batch 5:  Valid loss: 0.8054 Valid acc: 0.7290 Train loss: 0.4049 Train acc: 0.9500\n",
      "Epoch 177, CIFAR-10 Batch 1:  Valid loss: 0.8142 Valid acc: 0.7260 Train loss: 0.5756 Train acc: 0.7750\n",
      "Epoch 177, CIFAR-10 Batch 2:  Valid loss: 0.8107 Valid acc: 0.7318 Train loss: 0.4702 Train acc: 0.8250\n",
      "Epoch 177, CIFAR-10 Batch 3:  Valid loss: 0.8186 Valid acc: 0.7238 Train loss: 0.4850 Train acc: 0.9000\n",
      "Epoch 177, CIFAR-10 Batch 4:  Valid loss: 0.8183 Valid acc: 0.7270 Train loss: 0.4235 Train acc: 0.9000\n",
      "Epoch 177, CIFAR-10 Batch 5:  Valid loss: 0.8610 Valid acc: 0.7150 Train loss: 0.4309 Train acc: 0.9000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Valid loss: 0.8085 Valid acc: 0.7238 Train loss: 0.5227 Train acc: 0.8500\n",
      "Epoch 178, CIFAR-10 Batch 2:  Valid loss: 0.8194 Valid acc: 0.7244 Train loss: 0.4645 Train acc: 0.8000\n",
      "Epoch 178, CIFAR-10 Batch 3:  Valid loss: 0.8125 Valid acc: 0.7232 Train loss: 0.4223 Train acc: 0.9250\n",
      "Epoch 178, CIFAR-10 Batch 4:  Valid loss: 0.8109 Valid acc: 0.7298 Train loss: 0.4008 Train acc: 0.8750\n",
      "Epoch 178, CIFAR-10 Batch 5:  Valid loss: 0.8200 Valid acc: 0.7232 Train loss: 0.4121 Train acc: 0.9500\n",
      "Epoch 179, CIFAR-10 Batch 1:  Valid loss: 0.8081 Valid acc: 0.7230 Train loss: 0.5803 Train acc: 0.7750\n",
      "Epoch 179, CIFAR-10 Batch 2:  Valid loss: 0.8140 Valid acc: 0.7202 Train loss: 0.4931 Train acc: 0.8250\n",
      "Epoch 179, CIFAR-10 Batch 3:  Valid loss: 0.8235 Valid acc: 0.7210 Train loss: 0.4211 Train acc: 0.9500\n",
      "Epoch 179, CIFAR-10 Batch 4:  Valid loss: 0.8261 Valid acc: 0.7178 Train loss: 0.4238 Train acc: 0.9000\n",
      "Epoch 179, CIFAR-10 Batch 5:  Valid loss: 0.8341 Valid acc: 0.7188 Train loss: 0.4357 Train acc: 0.9000\n",
      "Epoch 180, CIFAR-10 Batch 1:  Valid loss: 0.7992 Valid acc: 0.7318 Train loss: 0.5844 Train acc: 0.8000\n",
      "Epoch 180, CIFAR-10 Batch 2:  Valid loss: 0.8208 Valid acc: 0.7302 Train loss: 0.4584 Train acc: 0.8750\n",
      "Epoch 180, CIFAR-10 Batch 3:  Valid loss: 0.8140 Valid acc: 0.7276 Train loss: 0.4017 Train acc: 0.9250\n",
      "Epoch 180, CIFAR-10 Batch 4:  Valid loss: 0.8142 Valid acc: 0.7300 Train loss: 0.4346 Train acc: 0.8750\n",
      "Epoch 180, CIFAR-10 Batch 5:  Valid loss: 0.8503 Valid acc: 0.7170 Train loss: 0.4095 Train acc: 0.9250\n",
      "Epoch 181, CIFAR-10 Batch 1:  Valid loss: 0.8100 Valid acc: 0.7298 Train loss: 0.5769 Train acc: 0.8000\n",
      "Epoch 181, CIFAR-10 Batch 2:  Valid loss: 0.7995 Valid acc: 0.7292 Train loss: 0.4334 Train acc: 0.8500\n",
      "Epoch 181, CIFAR-10 Batch 3:  Valid loss: 0.8280 Valid acc: 0.7186 Train loss: 0.4345 Train acc: 0.9500\n",
      "Epoch 181, CIFAR-10 Batch 4:  Valid loss: 0.8016 Valid acc: 0.7346 Train loss: 0.4047 Train acc: 0.8500\n",
      "Epoch 181, CIFAR-10 Batch 5:  Valid loss: 0.8184 Valid acc: 0.7274 Train loss: 0.4076 Train acc: 0.9000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Valid loss: 0.8227 Valid acc: 0.7224 Train loss: 0.5689 Train acc: 0.8000\n",
      "Epoch 182, CIFAR-10 Batch 2:  Valid loss: 0.7976 Valid acc: 0.7322 Train loss: 0.4305 Train acc: 0.8500\n",
      "Epoch 182, CIFAR-10 Batch 3:  Valid loss: 0.8041 Valid acc: 0.7316 Train loss: 0.4398 Train acc: 0.9000\n",
      "Epoch 182, CIFAR-10 Batch 4:  Valid loss: 0.8039 Valid acc: 0.7360 Train loss: 0.3871 Train acc: 0.9250\n",
      "Epoch 182, CIFAR-10 Batch 5:  Valid loss: 0.8031 Valid acc: 0.7296 Train loss: 0.4245 Train acc: 0.9000\n",
      "Epoch 183, CIFAR-10 Batch 1:  Valid loss: 0.8032 Valid acc: 0.7278 Train loss: 0.5497 Train acc: 0.8000\n",
      "Epoch 183, CIFAR-10 Batch 2:  Valid loss: 0.8262 Valid acc: 0.7208 Train loss: 0.4857 Train acc: 0.8250\n",
      "Epoch 183, CIFAR-10 Batch 3:  Valid loss: 0.8001 Valid acc: 0.7332 Train loss: 0.3816 Train acc: 0.9500\n",
      "Epoch 183, CIFAR-10 Batch 4:  Valid loss: 0.8265 Valid acc: 0.7236 Train loss: 0.4263 Train acc: 0.8750\n",
      "Epoch 183, CIFAR-10 Batch 5:  Valid loss: 0.8223 Valid acc: 0.7172 Train loss: 0.3999 Train acc: 0.9250\n",
      "Epoch 184, CIFAR-10 Batch 1:  Valid loss: 0.8207 Valid acc: 0.7258 Train loss: 0.6230 Train acc: 0.7750\n",
      "Epoch 184, CIFAR-10 Batch 2:  Valid loss: 0.8334 Valid acc: 0.7234 Train loss: 0.4536 Train acc: 0.8750\n",
      "Epoch 184, CIFAR-10 Batch 3:  Valid loss: 0.7909 Valid acc: 0.7308 Train loss: 0.3737 Train acc: 0.9500\n",
      "Epoch 184, CIFAR-10 Batch 4:  Valid loss: 0.8040 Valid acc: 0.7322 Train loss: 0.3996 Train acc: 0.9250\n",
      "Epoch 184, CIFAR-10 Batch 5:  Valid loss: 0.8137 Valid acc: 0.7266 Train loss: 0.4064 Train acc: 0.9250\n",
      "Epoch 185, CIFAR-10 Batch 1:  Valid loss: 0.8075 Valid acc: 0.7324 Train loss: 0.5500 Train acc: 0.7750\n",
      "Epoch 185, CIFAR-10 Batch 2:  Valid loss: 0.8217 Valid acc: 0.7300 Train loss: 0.4500 Train acc: 0.8500\n",
      "Epoch 185, CIFAR-10 Batch 3:  Valid loss: 0.8291 Valid acc: 0.7150 Train loss: 0.4111 Train acc: 0.9500\n",
      "Epoch 185, CIFAR-10 Batch 4:  Valid loss: 0.8090 Valid acc: 0.7310 Train loss: 0.4074 Train acc: 0.9250\n",
      "Epoch 185, CIFAR-10 Batch 5:  Valid loss: 0.8541 Valid acc: 0.7072 Train loss: 0.4409 Train acc: 0.9000\n",
      "Epoch 186, CIFAR-10 Batch 1:  Valid loss: 0.8087 Valid acc: 0.7264 Train loss: 0.5407 Train acc: 0.8250\n",
      "Epoch 186, CIFAR-10 Batch 2:  Valid loss: 0.8331 Valid acc: 0.7158 Train loss: 0.4541 Train acc: 0.8250\n",
      "Epoch 186, CIFAR-10 Batch 3:  Valid loss: 0.8055 Valid acc: 0.7260 Train loss: 0.3615 Train acc: 0.9250\n",
      "Epoch 186, CIFAR-10 Batch 4:  Valid loss: 0.7913 Valid acc: 0.7350 Train loss: 0.3810 Train acc: 0.9250\n",
      "Epoch 186, CIFAR-10 Batch 5:  Valid loss: 0.8110 Valid acc: 0.7324 Train loss: 0.4433 Train acc: 0.9500\n",
      "Epoch 187, CIFAR-10 Batch 1:  Valid loss: 0.8144 Valid acc: 0.7220 Train loss: 0.5687 Train acc: 0.8000\n",
      "Epoch 187, CIFAR-10 Batch 2:  Valid loss: 0.7924 Valid acc: 0.7356 Train loss: 0.4470 Train acc: 0.8750\n",
      "Epoch 187, CIFAR-10 Batch 3:  Valid loss: 0.8029 Valid acc: 0.7308 Train loss: 0.3742 Train acc: 0.9500\n",
      "Epoch 187, CIFAR-10 Batch 4:  Valid loss: 0.7952 Valid acc: 0.7382 Train loss: 0.3803 Train acc: 0.9250\n",
      "Epoch 187, CIFAR-10 Batch 5:  Valid loss: 0.7984 Valid acc: 0.7306 Train loss: 0.4090 Train acc: 0.9000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Valid loss: 0.8469 Valid acc: 0.7050 Train loss: 0.6060 Train acc: 0.7750\n",
      "Epoch 188, CIFAR-10 Batch 2:  Valid loss: 0.8138 Valid acc: 0.7302 Train loss: 0.4672 Train acc: 0.8250\n",
      "Epoch 188, CIFAR-10 Batch 3:  Valid loss: 0.8164 Valid acc: 0.7210 Train loss: 0.4050 Train acc: 0.9250\n",
      "Epoch 188, CIFAR-10 Batch 4:  Valid loss: 0.8307 Valid acc: 0.7240 Train loss: 0.3974 Train acc: 0.8500\n",
      "Epoch 188, CIFAR-10 Batch 5:  Valid loss: 0.8371 Valid acc: 0.7174 Train loss: 0.4586 Train acc: 0.9000\n",
      "Epoch 189, CIFAR-10 Batch 1:  Valid loss: 0.7858 Valid acc: 0.7352 Train loss: 0.5362 Train acc: 0.7500\n",
      "Epoch 189, CIFAR-10 Batch 2:  Valid loss: 0.8354 Valid acc: 0.7244 Train loss: 0.4512 Train acc: 0.9250\n",
      "Epoch 189, CIFAR-10 Batch 3:  Valid loss: 0.8212 Valid acc: 0.7204 Train loss: 0.4121 Train acc: 0.9000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Valid loss: 0.8221 Valid acc: 0.7228 Train loss: 0.4468 Train acc: 0.8250\n",
      "Epoch 189, CIFAR-10 Batch 5:  Valid loss: 0.8308 Valid acc: 0.7162 Train loss: 0.4150 Train acc: 0.9250\n",
      "Epoch 190, CIFAR-10 Batch 1:  Valid loss: 0.7973 Valid acc: 0.7294 Train loss: 0.5720 Train acc: 0.8000\n",
      "Epoch 190, CIFAR-10 Batch 2:  Valid loss: 0.7999 Valid acc: 0.7354 Train loss: 0.4793 Train acc: 0.8500\n",
      "Epoch 190, CIFAR-10 Batch 3:  Valid loss: 0.8273 Valid acc: 0.7216 Train loss: 0.4161 Train acc: 0.9500\n",
      "Epoch 190, CIFAR-10 Batch 4:  Valid loss: 0.8041 Valid acc: 0.7274 Train loss: 0.4418 Train acc: 0.8750\n",
      "Epoch 190, CIFAR-10 Batch 5:  Valid loss: 0.8171 Valid acc: 0.7218 Train loss: 0.4060 Train acc: 0.9000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Valid loss: 0.8005 Valid acc: 0.7268 Train loss: 0.5941 Train acc: 0.7500\n",
      "Epoch 191, CIFAR-10 Batch 2:  Valid loss: 0.8530 Valid acc: 0.7184 Train loss: 0.5030 Train acc: 0.8750\n",
      "Epoch 191, CIFAR-10 Batch 3:  Valid loss: 0.8184 Valid acc: 0.7302 Train loss: 0.4290 Train acc: 0.9250\n",
      "Epoch 191, CIFAR-10 Batch 4:  Valid loss: 0.8129 Valid acc: 0.7296 Train loss: 0.4186 Train acc: 0.8750\n",
      "Epoch 191, CIFAR-10 Batch 5:  Valid loss: 0.8075 Valid acc: 0.7302 Train loss: 0.4081 Train acc: 0.9250\n",
      "Epoch 192, CIFAR-10 Batch 1:  Valid loss: 0.8072 Valid acc: 0.7258 Train loss: 0.5499 Train acc: 0.7750\n",
      "Epoch 192, CIFAR-10 Batch 2:  Valid loss: 0.8235 Valid acc: 0.7264 Train loss: 0.4490 Train acc: 0.8750\n",
      "Epoch 192, CIFAR-10 Batch 3:  Valid loss: 0.7887 Valid acc: 0.7308 Train loss: 0.3805 Train acc: 0.9500\n",
      "Epoch 192, CIFAR-10 Batch 4:  Valid loss: 0.8459 Valid acc: 0.7100 Train loss: 0.4616 Train acc: 0.8750\n",
      "Epoch 192, CIFAR-10 Batch 5:  Valid loss: 0.8273 Valid acc: 0.7176 Train loss: 0.3945 Train acc: 0.9250\n",
      "Epoch 193, CIFAR-10 Batch 1:  Valid loss: 0.8022 Valid acc: 0.7298 Train loss: 0.5531 Train acc: 0.8000\n",
      "Epoch 193, CIFAR-10 Batch 2:  Valid loss: 0.8017 Valid acc: 0.7328 Train loss: 0.4020 Train acc: 0.9500\n",
      "Epoch 193, CIFAR-10 Batch 3:  Valid loss: 0.8181 Valid acc: 0.7240 Train loss: 0.4191 Train acc: 0.9250\n",
      "Epoch 193, CIFAR-10 Batch 4:  Valid loss: 0.8208 Valid acc: 0.7262 Train loss: 0.4308 Train acc: 0.9000\n",
      "Epoch 193, CIFAR-10 Batch 5:  Valid loss: 0.8168 Valid acc: 0.7296 Train loss: 0.4281 Train acc: 0.9500\n",
      "Epoch 194, CIFAR-10 Batch 1:  Valid loss: 0.7948 Valid acc: 0.7300 Train loss: 0.5767 Train acc: 0.8000\n",
      "Epoch 194, CIFAR-10 Batch 2:  Valid loss: 0.8129 Valid acc: 0.7222 Train loss: 0.4452 Train acc: 0.9000\n",
      "Epoch 194, CIFAR-10 Batch 3:  Valid loss: 0.7954 Valid acc: 0.7322 Train loss: 0.4078 Train acc: 0.9500\n",
      "Epoch 194, CIFAR-10 Batch 4:  Valid loss: 0.8095 Valid acc: 0.7296 Train loss: 0.4062 Train acc: 0.9000\n",
      "Epoch 194, CIFAR-10 Batch 5:  Valid loss: 0.7989 Valid acc: 0.7366 Train loss: 0.3965 Train acc: 0.9500\n",
      "Epoch 195, CIFAR-10 Batch 1:  Valid loss: 0.7866 Valid acc: 0.7332 Train loss: 0.5331 Train acc: 0.8250\n",
      "Epoch 195, CIFAR-10 Batch 2:  Valid loss: 0.7995 Valid acc: 0.7384 Train loss: 0.4151 Train acc: 0.8750\n",
      "Epoch 195, CIFAR-10 Batch 3:  Valid loss: 0.8010 Valid acc: 0.7292 Train loss: 0.4133 Train acc: 0.9250\n",
      "Epoch 195, CIFAR-10 Batch 4:  Valid loss: 0.7996 Valid acc: 0.7332 Train loss: 0.3696 Train acc: 0.9500\n",
      "Epoch 195, CIFAR-10 Batch 5:  Valid loss: 0.8240 Valid acc: 0.7212 Train loss: 0.3847 Train acc: 0.9250\n",
      "Epoch 196, CIFAR-10 Batch 1:  Valid loss: 0.7975 Valid acc: 0.7312 Train loss: 0.5494 Train acc: 0.8000\n",
      "Epoch 196, CIFAR-10 Batch 2:  Valid loss: 0.8053 Valid acc: 0.7322 Train loss: 0.4650 Train acc: 0.8750\n",
      "Epoch 196, CIFAR-10 Batch 3:  Valid loss: 0.8345 Valid acc: 0.7100 Train loss: 0.4302 Train acc: 0.8750\n",
      "Epoch 196, CIFAR-10 Batch 4:  Valid loss: 0.8404 Valid acc: 0.7138 Train loss: 0.4466 Train acc: 0.8500\n",
      "Epoch 196, CIFAR-10 Batch 5:  Valid loss: 0.7910 Valid acc: 0.7370 Train loss: 0.3812 Train acc: 0.9250\n",
      "Epoch 197, CIFAR-10 Batch 1:  Valid loss: 0.8156 Valid acc: 0.7254 Train loss: 0.5632 Train acc: 0.8000\n",
      "Epoch 197, CIFAR-10 Batch 2:  Valid loss: 0.8145 Valid acc: 0.7330 Train loss: 0.4335 Train acc: 0.8250\n",
      "Epoch 197, CIFAR-10 Batch 3:  Valid loss: 0.8050 Valid acc: 0.7246 Train loss: 0.3753 Train acc: 0.9500\n",
      "Epoch 197, CIFAR-10 Batch 4:  Valid loss: 0.8236 Valid acc: 0.7274 Train loss: 0.4433 Train acc: 0.8500\n",
      "Epoch 197, CIFAR-10 Batch 5:  Valid loss: 0.8165 Valid acc: 0.7262 Train loss: 0.4289 Train acc: 0.9250\n",
      "Epoch 198, CIFAR-10 Batch 1:  Valid loss: 0.8090 Valid acc: 0.7298 Train loss: 0.5611 Train acc: 0.8500\n",
      "Epoch 198, CIFAR-10 Batch 2:  Valid loss: 0.8313 Valid acc: 0.7202 Train loss: 0.4429 Train acc: 0.8250\n",
      "Epoch 198, CIFAR-10 Batch 3:  Valid loss: 0.7955 Valid acc: 0.7308 Train loss: 0.4149 Train acc: 0.9250\n",
      "Epoch 198, CIFAR-10 Batch 4:  Valid loss: 0.8434 Valid acc: 0.7090 Train loss: 0.4309 Train acc: 0.9250\n",
      "Epoch 198, CIFAR-10 Batch 5:  Valid loss: 0.8100 Valid acc: 0.7326 Train loss: 0.4297 Train acc: 0.9250\n",
      "Epoch 199, CIFAR-10 Batch 1:  Valid loss: 0.7933 Valid acc: 0.7298 Train loss: 0.5462 Train acc: 0.8250\n",
      "Epoch 199, CIFAR-10 Batch 2:  Valid loss: 0.8655 Valid acc: 0.7160 Train loss: 0.4565 Train acc: 0.8250\n",
      "Epoch 199, CIFAR-10 Batch 3:  Valid loss: 0.8105 Valid acc: 0.7248 Train loss: 0.3945 Train acc: 0.9500\n",
      "Epoch 199, CIFAR-10 Batch 4:  Valid loss: 0.8113 Valid acc: 0.7314 Train loss: 0.4200 Train acc: 0.8500\n",
      "Epoch 199, CIFAR-10 Batch 5:  Valid loss: 0.8038 Valid acc: 0.7248 Train loss: 0.3981 Train acc: 0.8750\n",
      "Epoch 200, CIFAR-10 Batch 1:  Valid loss: 0.8132 Valid acc: 0.7210 Train loss: 0.5517 Train acc: 0.8500\n",
      "Epoch 200, CIFAR-10 Batch 2:  Valid loss: 0.8041 Valid acc: 0.7386 Train loss: 0.4612 Train acc: 0.8750\n",
      "Epoch 200, CIFAR-10 Batch 3:  Valid loss: 0.7897 Valid acc: 0.7354 Train loss: 0.3803 Train acc: 0.9750\n",
      "Epoch 200, CIFAR-10 Batch 4:  Valid loss: 0.8092 Valid acc: 0.7288 Train loss: 0.4081 Train acc: 0.9250\n",
      "Epoch 200, CIFAR-10 Batch 5:  Valid loss: 0.7990 Valid acc: 0.7280 Train loss: 0.3849 Train acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            # only print stats for each epoch\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.71513671875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiEQ0ORYRERERifx/BFUmoUoBuaMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3b03cc160>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
